# 2017-09-01

## Handling Dead Player Agents

I'd really like to move them to a separate list.
Maybe even separate object types.

The expectation is that they will be rendered differently.
For example, the agent sprite might be replaced with a brief explosion animation.

If corpses are displayed, they should be on a layer below where the player agents are drawn. That would give us *three levels* now:

1. Corpses
2. Player agents
3. Player agent decorations

Then we handle death by constructing a new DeadAgent object with the same name as the former PlayerAgent. It might have the same sprite, even.
For now, since there are no sprites, we just want to draw a gray circle, the name of the bot, and its aim direction, mainly as debugging aids.
Then maybe implement animation by just having the "sprite" fade away.

Note that this means we probably want to draw the name on the bottom layer, 
as well as the disc.

So continuing with the death handler:

1. Construct a new DeadAgent object from the dead PlayerAgent.
2. Remove the agent's body from the physics world.
3. Remove the agent sprite from the sprite list.
4. Add the dead agent to the dead agents list.
5. Render dead agents, sprite discs, and sprite decorations, in that order.

Someday we might want exploding bots to be a hazard to other bots, in which case there will be a time when the dead bot is still involved in collisions.

The big question is whether we really want a separate dead bot class, 
or rather a single bot class that can be in multiple states.
I am now thinking the *state machine* approach is probably a better one.

Currently there's only two states an agent can be in: `Alive` and `Dead`.
But later there could be lots more. 
Even the `Dead` state might want to be split into `Dead1`...`Dead8` or whatever, to handle gradual elimination from the game. For example, if we want the appearance to change gradually, or we want explosion damage to decrease over time.

That leads to the question of how agent state interacts with animations.
A *state* might actually be a complex object capable of internal transitions.
In that case, it needs a final transition to another meta-state. 
For example, what does `Dead.8` transition into?

There can only be one runner responsible for state transitions.
It queries the state for its next transition, given environmental data (e.g., a reference back to the controller). The state may respond "Done!", in which case what would we do next?

There has to be a top level transition table that says "Enter meta-state `Dead`; on `Dead.done`, enter meta-state `Eliminated`," or something like that.

The idea is that on entering state `Dead`, there is an assignment like:
```
sprite.stateData = {counter: 0};
```
And the controller enters a new state by recursively calling something like 
`stateData.next()`.

This means we are running an RTN, but hopefully one with a very strictly bounded stack.

So stuff happens to an agent according to its current state.
Then after all that, we check to see if its state changes.
Then more stuff happens according to the updated state.
Both `update` and `render` would be sensitive to the agent's state.

There has to be a uniform state API.
Currently that is just a single `next()` method.

The main problem is that if there is just a single `state` property that we plug complex sub-states into, they don't know what to transition to when they are done. 
There really has to be a *stack* of states. So the top-level `next()` can call the embedded `next()`, and if it returns "Done!", then the top-level transition table says what to replace the sub-state with.

So there's `PlayerAgent.prototype.next`, and also `PlayerAgent.prototype.state`, which is inhabited by an object that also has a `next` method.
```
next = () => {
	let rslt = this.state.next();
	while (rslt === Q_DONE) {
		this.state = State.create(this.delta(this.state.id));
		this.state.next();
	}
}
```
So state class `Alive` would only return `Q_DONE` when the agent's health dropped to zero or less. Then it would transition to `Dead`.
And state class `Dead` would only return `Q_DONE` after an internal counter had reached a predefined limit. Then it would transition to `Eliminated`.
State `Eliminated` would trigger cleanup of the agent.
Meaning some top-level game logic in `update` would have to ask "if X is in state Eliminated, remove it from the list, or always skip it in update and render, or whatever."

The state API might need to include its own `update` and `render` methods as well.
I guess `update` is really where it would make its own internal transitions.

Changes in health will happen in *collision handlers* (mainly when agents collide with weapons and ordnance). These will be called by the physics engine during its `update` step. And that, in turn, happens at the very end of the main `update` step.
Taken all together, that means that an agent will actually die in between the end of the update step and the beginning of the next render step.
That doesn't seem right...

If you die during the update step, then we want you to appear dead in the immediately following render step.
To make that happen, we would have to do the physics step, then the sprite step.
So the new order is:

1. Advance physics simulation
2. Bot update: execute program for 1 chronon (if in state Alive)
3. Render

It's easy to imagine other states, especially as ways of implementing debuffs.
Being stunned or slept would be just like being dead, but on `Q_DONE` you would transition back to `Alive`.
But for right now we just have two states: Dead and NotDead.

### StateDead

I guess the only property of a dead agent is a counter to say when to clean up.
So more or less:
```
next: () => {
	if (--counter === 0) {
		return Q_DONE;
	}
	return Q_NOT_DONE;
}
```

In the long term, StateDead might come with its own sprite animation.
And it certain comes with a unique state id (`Q_DEAD`, presumably).

Oh, and I guess it needs a `position` property, since that's no longer available from the physics body. Though in fact there is no longer any reason to destroy the physics body, so maybe that's not a problem going forward.

### StateNotDead

This would have a `Matter.Body` member, and a `health` property, at the very least. Currently we also have stuff like `aim` and position and motor vectors, which are really redundant with the body data.
There will be lots more later, corresponding to various agent "loadouts".
More importantly, there will be a `program` property.

An update step for a not-dead agent will involve:

1. Checking state and transitioning if necessary
1. Running the program for `cpuSpeed` cycles
2. Updating the physics body properties accordingly

It may be that the program is allowed to update physics properties directly.

Ultimately there may be a two-step process where running the program just creates a log of side-effects, and a second stage actually evaluates the log and changes the world accordingly.

At one point I was thinking of running all active programs in parallel.
That would not be possible if agents are updated one after another.
We would have to split the master `update` function to loop through the sprite list three times:
```
update: () => {
	for (var i=0; i< n; ++i) {
		sprites[i].updateState();
	}
	interpreter.advance(sprites);
	for (var i=0; i< n; ++i) {
		sprites[i].playLog();
	}
}
```
Something like that.
Is there still any reason to do this?

No sprite can move during a chronon. 
So even though you can move your turret, it will only register sprites in the state they were in before the update.
You can fire, but your attack will only be launched at the end of the chronon.
That is, a new ordnance sprite will be created and added to the projectiles list and the physics world, but it won't start moving until the next physics update.
So all projectiles will appear to happen simultaneously, I think.

Again, all events that can damage you happen during the physics simulation step.
If we need to refine things, then we need to have several short-step/logic iterations in between frame rendering.

### StateGarbage

After you've been dead for a little while, you become garbage, which needs to be made to disappear. 
Either we actually remove all sprite data from the game, 
or we leave it like this and make sure our loops skip over all garbage sprites.
They are completely invisible and undetectable, and do no work of any sort. 
They just take up memory.
And this is a state that never emits `Q_DONE`. It's like a sink state in automata theory.


## Programming Language

I notice that JVM bytecode is a stack based language, like RoboTalk.
It doesn't look like there's a lot of `DUP`, `ROLL`, `DROP` instructions, though. Some of them exist, but don't seem to be used a lot.
Actually, on closer reading, there's a lot of finicky `dup` instructions, so I guess it isn't enough to drive people away from stack machine programming.

The key feature is *event handlers*, aka *interrupts*.

Suppose we have an event handler for colliding with a wall.
That's code that exists in the program, but the event itself happens during physics simulation.
That means that the actual event handler, the one registered with `Matter.Events`, is going to just add an event to a queue or put a message in a bot's mailbox, or something like that.

Now, it's possible to be in collision with more than one thing at once.
In a corner, you are hitting two walls. You may bump more than one other player agent. And so on.
So there's going to be multiple events, even of the same type.
Do you handle them sequentially? Only handle the first one?

First off, different events may have different *priorities*.
So on returning from your priority 1 event handler, you may immediately get bounced to your priority 2 event handler.

There should be an option to drop the remainder of the event queue.

Maybe the way to implement a collision handler is to get a list of *all* the objects you are in collision with.
That's the only reliable way to compute your best escape vector, for example.

In a stack machine, you'd have to do something like have the array length at the top of the stack.
And you'd have to have instructions for pushing the item at *stack - N*.
Also, returning from the handler would have to unwind the stack entirely.
So the arguments form a sort of *stack frame*, I guess kind of like in the JVM?

The actual items on the stack would have to be some kind of *object references*, where the objects would live somewhere else. 
Note that now we need some sort of *garbage collection*.
In this particular case, since the objects are created by the game engine, and are not needed after the event has been handled, they could actually be included in the stack frame, or automatically destroyed as soon as their reference goes out of scope.

Note that if `goto` is allowed from inside an interrupt handler to outside it, then we can be in very serious trouble.
We'd have to be able to analyze all `goto` targets and determine which ones would cause an exit from the current block.

### Collisions

Are collisions the only game events that get reported by the physics engine?
So far, I think that is the case.

The engine gives us a list of affected pairs. 
In addition, there are two distinct events that we may want to collapse.

1. `collisionActive`
2. `collisionStart`

So we have to scan the lists of affected pairs for both types, 
and deliver the relevant records to each affected object's event queue.
Ideally they would be ordered by *timestamp* as well,
but for now we may as well consider events occurring in the same engine step to have been simultaneous.

Maybe we can just react to `collisionActive` events?

The information we want to deliver with each collision:

1. Who/what we are colliding with.
2. Direction of the collision?
3. Force of the collision?

Not exactly sure how we calculate these.
Really, the most important direction is the direction to the other sprite, and we get that from the sprite itself.

So there has to be a way to register a handler for a given event.
The VM will come with a set of fixed memory locations that will hold code addresses. So something like:
```
STORE collision, OnBump
```
Where `OnBump` must be the label of an address.
It should also be possible to *compute addresses* and store the result of the computation.

### Memory Model

So, apart from the stack, there will be a collection of fixed memory locations, AKA *registers*, where we can store stuff.
Among these would be the *interrupt handlers*, 
and *hardware registers* like `aim`.
There also needs to be some sort of user memory.
I guess any symbol that isn't a register name or an interrupt name could be interpreted as a memory location.

It should also be possible to declare a constant data area.
That is, the user should be able to declare named constants, for code readability.

I guess we can always use tagged symbols.
Like `@foo` is the address of label `foo`; `@(foo + 6)` is the address 6 after `foo`, etc.
And maybe `!collision` is an interrupt handler register,
and `&aim` is a register, I dunno.

In RW it was necessary to distinguish literal register names from the value stored there, so you could store a register name in another register.
Maybe that will be necessary here as well?


# 2017-09-02

## Programming Language

The main goal is to have a language that makes it clear how many "CPU cycles" a given program fragment is going to consume.
So the simplicity of assembler, bytecode, and three-address code is appealing.
But we do not require the efficiency,
nor do we need to worry too much about how many real cycles our interpreter is taking up per instruction. 

For example, there's a lot of work going on in a function call: stack frame allocation, pushing parameters, saving the return address, and so on.
And compiling an access via a path through a struct leaves us with something quite simple at run-time, but if any part of that path involves de-referencing a pointer, then suddenly there's more instructions.
Maybe we don't care so much, here?

Also, in TAC, we'd have to compile out an array access into a number of steps, possibly. But we can just say "`[]` is an operator, like any other".

### Are registers just storage locations?

I've written register storage as two steps:
```
* 		driveX 	-1 		_T4
Store 	driveX 	_T4 	_
```
This is because, in the back of my head, I know that storing to the drive register is a method call, not simply an assignment. 
But that's under the hood. How complicated would the interpreter become if we allowed direct storage in TAC?
```
*	driveX	-1	driveX
```


## Garbage Collection

Note that we do not want to just transition bullets to state `Eliminated` and then leave them around and just skip over them. 
Memory will leak away really quickly this way.
Bullets definitely want to be deallocated once they have died.


## Classes and Modules

We're getting to a point where the PlayerAgent code wants to be split off in its own separate module.
What other modules do we want?


## States

If states handle their own rendering, then the state API includes both `update` and `render`. 
At that point, it starts to look a lot like `PlayerAgent` itself...
```
update = () => {
	let rslt = this.state.update();
	while (rslt === Q_DONE) {
		this.state = State.create(this.delta(this.state.id));
		this.state.update();
	}
}
```
I guess for `update` there's still the question of who handles top-level transitions. 
But for example the `StateNotDead` object is now the thing that actually runs the interpreter.

I guess to some degree it's the old question of whether to implement variations in behavior by inheritance or aggregation.

Anyhow, the point is that now top-level `render` is invariably trivial:
```
render = () => {
	this.state.render();
}
```

Perhaps it is still the case that the player agent *model data* is all still held in the main PlayerAgent object.
And so the methods that manipulate it (getters and setters and their ilk) also belong to the main object.
It's really just `update` and `render` that get delegated.

What about top-level `delta`?
```
delta = (id) => {
	switch (id) {
		case State.Q_NOT_DEAD:
			return State.Q_DEAD;
		case State.Q_DEAD:
			return State.Q_ELIMINATED;
		case _:
			// Eh? Nothing else should ever be Q_DONE!
			break;
	}
}
```
I guess that's more or less what it would look like.

I guess initial state is always `StateNotDead`, though I suppose it could someday be a special `StateInitializing` or something like that, for one time start up processing.


# 2017-09-03

## Vision and Aiming

I was thinking of just using the `aim` angle for both vision and shooting.
But it might be better to keep them separate.
For example, if you see an enemy at point V1, but you want to lead your shot, then you have to look away, maybe messing up your tracking routine.

> In other words, it's important to be able to do shot-leading and 
  object tracking at the same time.

And that, in turn, means that their memory systems have to be distinct.
A different set of registers.

Right now, I am not interested in shooting, just vision.
Maybe call it the `look` angle, not `aim`.

Also, the vision system ought to be capable of replacement.
For bots, it can be a single ray, like in robowar.
But for humanoid dungeon explorers, it will want to be a cone, or a fan of cones, or something.

Note that Matter's ray-intersection query is supposedly implemented by doing collision detection with a very thin rectangle. 
If that's so, then really there's no deep difference with using collision detection with a triangle or pie slice.

Next question: Can bullets prevent you from seeing an enemy?
I kind of think not.
So, like robowar, we might distinguish routines that look for enemies from routines that look for bullets.

### Implementation

I guess I want to trigger an interrupt every time `look` changes to a new angle, and there is an enemy visible along that ray.

So the code for `setLook` has to test to see if the agent has an interrupt set for the `look` register, and branch to it if a `Matter.Query.ray` test comes back positive. 

Also the collisions returned by Matter have to be reduced to a single nearest object.

Someday there may be an interrupt parameter that limits the distance.

Also and in any case the distance is an important thing to know, and maybe our hardware can work that out for us.

For now, we expect the interrupt handler to be a JavaScript function object living at `agent.state.prototype.on<Event>`.
So `setLook` would check to see if that property exists and is defined, and, if so, it would call it, with the sprite that triggered the interrupt.

The RW design of the aim register and friends has them be readable as well as writable. 

* Writing changes the angle
* Reading returns the distance to the nearest thing, or zero (I think)

So you can say:
```
let o = getAim();
if (o) {
	// fire!
}
```
But now how do you read the look angle?
Like if you want to set `aim = look`?
Maybe make the register complex?

* `look.angle`: writable and readable
* `look.thing`: read-only

It would be nice if `getLook` returned `look.thing`, so you could still write:
```
if (getLook()) {
	// ...
}
```

# 2017-09-04

## Vision

One problem with `Matter.Query.ray` is that it needs a collection of bodies to search among. 
And that, in turn, entails access to the physics engine.
But if `setLook` is a method of PlayerAgent, then we have a circular dependency, because the main code needs PlayerAgent to have been loaded, and now PlayerAgent needs the main code to have been loaded...

Maybe factor out the physics stuff? Is that possible?
It is possible, but it means that `matterEngine` is now a public member of the `MAGIC` namespace.

But we still need some way to get the list of agent bodies from the engine.
This list is only populated in the main routine. 

Maybe I need to be thinking less in terms of objects with accessor methods, and more in terms of global functions. 
So instead of `agent.setLook(...)`,
use `MAGIC.setLook(agent, ...)`.

It looks like some things have to be declared early, 
before they are populated.
So more than just the physics interface needs to be extracted.


## Game Data

Maybe we need to define a general GameData object before we populate it.
It could contain the `arena` and the `sprites` list,
and eventually also the `projectiles` list, 
and the map.

### Agents

Should this be a list, or something indexed?
E.g., by agent name.


## Aside on Quadrants

If our aim is close to 90 degrees, then maybe the (+,+) quadrant is not what
we want. Maybe try a *diagonal* quadrant system. 
Which quadrant system you use depends on which mutliple of 45 degrees the ray angle is closest to.

( 45, 135)	y > x && y > -x  <==>  y > abs(x)
(135, 225)	y < -x && -y < -x  <==>  abs(y) < -x
(225, 315)	y < -x && y < x  <==>  y < abs(x)
(315, 45)	y < x && y > -x 
			-y > -x && y > -x  <==>  abs(y) > -x

## Ray Casting

Looks like Matter's ray query is flukey. 
It is still turning up sightings of dead bodies...


# 2017-09-05

## Ray Casting (Cont'd)

Found a really nice looking algorithm for line-circle intersection, which even deals with the tricky case of line segments.
It looks like it only takes a single square root, so it should be pretty efficient.
Stack Overflow URL:
```
https://stackoverflow.com/questions/1073336/circle-line-segment-collision-detection-algorithm
```
Parameters:
```
Taking

    E is the starting point of the ray,
    L is the end point of the ray,
    C is the center of sphere you're testing against
    r is the radius of that sphere

Compute:
d = L - E ( Direction vector of ray, from start to end )
f = E - C ( Vector from center sphere to ray start ) 
```
And the code:
```
float a = d.Dot( d ) ;
float b = 2*f.Dot( d ) ;
float c = f.Dot( f ) - r*r ;

float discriminant = b*b-4*a*c;
if( discriminant < 0 )
{
  // no intersection
}
else
{
  // ray didn't totally miss sphere,
  // so there is a solution to
  // the equation.

  discriminant = sqrt( discriminant );

  // either solution may be on or off the ray so need to test both
  // t1 is always the smaller value, because BOTH discriminant and
  // a are nonnegative.
  float t1 = (-b - discriminant)/(2*a);
  float t2 = (-b + discriminant)/(2*a);

  // 3x HIT cases:
  //          -o->             --|-->  |            |  --|->
  // Impale(t1 hit,t2 hit), Poke(t1 hit,t2>1), ExitWound(t1<0, t2 hit), 

  // 3x MISS cases:
  //       ->  o                     o ->              | -> |
  // FallShort (t1>1,t2>1), Past (t1<0,t2<0), CompletelyInside(t1<0, t2>1)

  if( t1 >= 0 && t1 <= 1 )
  {
    // t1 is the intersection, and it's closer than t2
    // (since t1 uses -b - discriminant)
    // Impale, Poke
    return true ;
  }

  // here t1 didn't intersect so we are either started
  // inside the sphere or completely past it
  if( t2 >= 0 && t2 <= 1 )
  {
    // ExitWound
    return true ;
  }

  // no intn: FallShort, Past, CompletelyInside
  return false ;
}
```

There might be simpler solutions if I just need a yes/no answer, but if I need the actual intersection points (e.g., for doing my own collition resolution), then this looks pretty good. 

It does not return the distance, I think: that would cost another square root. 
But if we already know the view length, then that gets to be part of the *L* calculation. So a maximum distance is built in (at the cost of computing *L*, which is simple but not easy).

### An Alternative Simple Predicate

Suppose that we think of the vision system not as being a ray, but a rectangle.
Take the diameter perpendicular to the aim vector, and the maximum vision distance, and form a rectangle. If the center of any agent is inside that rectangle, then that agent is visible.

That doesn't necessarily help us figure out which one is *closest*, but it does tell us which small subset we have to sort through.

This assumes that all agents are the same size.

Also, we have to find two points on the observer agent's circumference, 
so that's a bunch of trig calls, I think, before you even have your rectangle.



## Code Organization

This still seems to me to be a little wonky.

One thing in JSRoboWar that I liked was treating even such simple robots as composites, for drawing purposes. Draw the body, then draw the turret.
Also, though I think this was forced by the drawing library, using `translate` (for the body) and `rotate` (for the turret), rather than doing the dirty calculations locally.

Anyway, at the root of the theoretical object tree would be the *Game*.
This is really just:

* A place to dock top level game components
* A means to handle communication and coordination of top components

Among our top things would be the *GameData*, 
the visualization system (*GameView*?),
and any controls linked to the main page (start, stop, load agents).
If I'm using a physics engine, then there is also that.
That is, there is the agent, making internal decisions and activating its available controls, and then there is the world it lives in, handling its own forces (principally momentum).

But each separate game object has realizations in all of these systems.
An agent has logic that maps game data to game data,
an appearance that is handled by the game view,
and a body that is handled by the physics engine.
And they all communicate: the appearance depends on the game state of the agent, for example, and the data about its position cannot be determined without consulting the physics engine.

I think the main game loop still looks like this:
```
loop: () => {
	window.requestAnimationFrame(loop);
	physics.update();  // where am I?
	psyche.update();   // where should I go?
	appearance.update();  // what do I look like?
}
```
That's really just a variation on the standard update-render loop, 
with the additional complication of two things to update.

The calls to `update` and `render` are recursive.
That is, every game object probably has an `update` and `render` method, 
and also every object has some sort of *parent container*, leading eventually to the top level game loop.

### Aside on Models, Views, Controllers, and Events

Suppose we take the *model* to be pure data,
and the *view* to be pure drawing operations and properties.
Then one way to look at a *controller* is that it is something that responds to *events* by modifying the model. 
The view is always and only derived from the model.

Then the whole issue becomes one of "Where do these events come from?"
In a web app, events generally come from the screen, more or less, or at least they appear to. There are mouse clicks and key presses, and often we need to know a screen coordinate for where they happened.
So the "view" can *appear to be* also the "controller".

For example, if I click a button, there is a mouse click event, 
and the system has to decide what system object to hand off that event to.
The answer has to do with what object contains that event's (x,y) coordinates.
So it has a visual, or at least spatial, component.

And then that *smallest enclosing shape* becomes the object that actually now appears to be the event source.

The controller, meanwhile, is responsible for mapping event data to changes in the actual model data. 
In a way, it's like the model is a pure struct, and the controller superimposes a set of functions on top of that struct, so as to maintain consistency and propagate consequences.
So the controller is a set of event handlers which then calls these methods according to the core application logic.

Meaning that now the controller, not the model, is the locus of application logic.
I think this is a bit non-standard...

If the model of a scroll bar is something like a document line number,
then the controller would map mouse events in the various bar regions to changes in that line number. Mostly relative changes -- I think the controller would not generally need to know what the actual line number was.
And I suppose this could have knock on effects, like certainly for the document window widget, and maybe for the selection and so on.
Is that done by direct calls, or by emitting new events for other controllers to consume?

Probably the scrollable window has its own controller, because how else could consistency be maintained between the document view, selection highlight, and scroll slider be maintained?
(Consider for example the effect of a down-arrow key, which sometimes might trigger scrollbar movement.)
So events aren't necessarily just publish and subscribe, but rather things that go up the tree, and then back down. Always following ancestor-descendant paths.

So what's our analog here?
In general, there's nothing quite like a mouse-click. Events are all generated from within the game.

* The physics engine triggers *collision events*
* The turret system triggers *sighting events*

And, again, there can be whole event cascades:

* Collision with a projectile causes health to drop to zero.
* That causes a state change.
* That might introduce new projectiles (burning shrapnel),
  and new collisions.

Of course, not all in the same game cycle.

### How does animation work?

It would be nice if we could just submit animations to a separate drawing engine that would handle all that.

Meaning, it is the drawing engine that is responsible for advancing the animation frame and cycling back if it's an animation loop.
The game engine just says: "Okay, we just changed direction, so tell the renderer that we are now using animation *WalkingNorthEast*!"

That would be a huge load off of the game engine.
And it makes it easy to implement a machine instruction to switch animations (similar to RoboTalk `ICON0` and friends, only for animations).

Indeed, we could say that a *sprite* is actually an animation, a sequence of sprites, really. So have instruction `SPRITE0`, basically.

With certain sprite settings happening automatically? Like you can set direction animations to be triggered automatically on change of direction?

All this really means is that a call to `agent.render()` is going to not only draw a sprite, but also update the animation frame counter.

Note that we are probably going to have a single *walking* animation, and simply rotate the sprite in the current direction of travel.
That works for top-down perspectives.
If we move to isometric, I believe things get more difficult.
Then maybe movement becomes tile-based? Maybe with small tiles?

### Communication

For example, right now an agent has a `body` member that is a reference to its physics body, and the physics body in turn has a `spriteIndex` member so as to reach back.
We need this for things like the brain knowing what our current X and Y positions are, by asking the body.
And the body knowing what our current desired velocity is by asking the brain.

Right now this is implemented (as noted) by giving everybody direct pointers to each other, so that any method or member can be accessed and even updated.
Arrays of references (even if they are just indexes) will almost certainly lead to *dangling references* as things get destroyed and removed from the game.
Also I think *memory leaks*, as circular references can subvert garbage collection.

So, the big question is whether we have three trees, of bodies, brains, and sprites, or a single tree of objects that know how to manage all of the above.
And we know in advance that bodies and everything else are in different trees, cause that's the way the physics world works.
And if we use a rendering library as well, then probably that will take over all the sprites.
I guess that kind of answers things for us: unless I want to write a physics engine, we are going to have to have two things owning references to an object's body: the physics engine and the logic engine.

It would be nice if a recursive call to `update()` would return a list of "things to tell the physics engine," and "things to tell the renderer."
Then everything is based on ID, and the respective master object just has to handle things like IDs no longer existing in as elegant a way as possible.

Suppose the physics engine changes our position.
How do we tell the renderer?
The physics update will not cooperate in giving us a nice list of changes, at least not on its own. 
But I suppose we could engineer some sort of *wrapper*. 
That would have to query all bodies for their current X and Y coordinates, 
and perhaps also their collision status, and make a list to submit to the logic updater and the renderer.

Note that it is very important that the logic updater cannot change an agent's position. Only the physics engine can do that.
So the position input to the engine and to the renderer is the same.

The renderer does not result in any list of changes.
It is a pure subscriber to the model. 
So it's really just the physics and logic talking back and forth that we have to manage.

One interesting problem to consider is how to handle **projectile impacts**.
If a bullet hits a bot, the bullet vanishes from the physics world,
but some kind of explosion or blood spatter or something might still take a couple more animation frames to complete.
So the sprite still exists.

Projectiles in general are interesting, in that they have no internal logic.
They just change position, and collide with things (mostly walls).

So the physics wrapper provides us with a list of all the currently active *bodies*, which is not exactly equal to the currently active *sprites*.
It tells us their current coordinates, and also lists any active collision pairs.
(So it's not just a list of bodies; it has some internal structure.)

> Aside: The physics wrapper will probably have to query newly created bodies
  for their internal IDs to pass on to their models.

So actually what it does is return a list of *updates*, which include the positions of all bodies, and also collision events.

That's the input to the logic updater, as well as part of the input to the renderer.

> The other part of the renderer input is a list of sprite changes from the 
  logic engine, but that should be infrequent.

The logic updater can then run through this list and update registers accordingly. 

* Coordinate updates only matter for *agents* (player or otherwise).
  We then have to map the body ID to a model ID, and set the relevant registers.
* Collisions matter to everybody. 

Note that if a projectile collides with a wall, it is removed from the game.
Or the physics part, anyway. Its sprite might still hang around, in the form of a brief impact animation, but the rest of it is gone.
It is possible for the physics wrapper to recognize this situation and take action on its own, but that seems like it would break encapsulation.

We respond to collisions involving agents by putting notices on the agents' event queues. So we fill up the event queues at the top level, before going into the agent-level update loop.

Anyway, then the logic module runs and produces its own list of updates for the renderer. I think the only information that the logic module can produce that would be relevant for the renderer is a case where a new animation needs to be played. That is, a change of state that has a visible consequence.

Take the case where the model executes a `STRIKE 5` ("strike with energy 5") instruction. We might want the renderer to animate a sword swipe.
It might need even more information than that: maybe the currently equipped weapon is an axe, or a spear (which stabs instead of swiping). 
I guess there's a whole collection of weapon strike animations and someone has to pick which one to use.

The logic module doesn't know what a strike looks like, or if there even is an existing animation for a strike for this particular kind of agent.


### Removing Objects from the Game

In the case of a projectile hitting a wall, we know already from the collision record that the projectile must be removed from the physics world. 
So maybe the physics wrapper could handle that itself.
The collision is still important, because it may trigger a special animation, and at very least it requires removing a sprite from the graphics world.
So we still leave that in the list of updates.

Actually, any collision involving a projectile will cause that projectile to disappear. Again, possibly with a short exit animation.

But this represents a "responsibility leak".
Because cases where we have to remove an agent from the physics world only occur, or are only recognized, within the logic world.

And then finally there's the graphics world. 
How do we handle exit animations?
We have to know that, once we hit the last frame, we have to remove the sprite from the graphics world entirely.
I think in all cases once we start that exit animation, the corresponding body and brain have already been removed from their worlds.

What about explosions? 
Really in this case we are *replacing* one body with another, or a cloud of others. Importantly, it's still a physical body, and we still want to track collisions with it.
But it has a totally different density and so on.
Really, it has no place in the physics world at all, except as something that may be collided with. But the collision shouldn't cause any big changes in the trajectory of the colliding object.
So in this case we remove the original object from *all worlds*, and add a new object that may have a body, certainly has a sprite, and probably doesn't have anything much in the way of logic.

Note that if we have a wrapper around the physics world, 
then it should be possible to add our own special cases to it. 
So we might handle the physics of explosions with a totally different engine, with its own update function.

## Vision, Revisited

So we still have this communication problem. Namely that every update to the `aim` register or its cousins requires us to cast a ray in the physics world.

That's not *strictly* true. If the logic world knows X and Y positions of things, then we have all the data we need to resolve vision queries.
But it feels like another *responsibility leak* to me.

There's also the symmetrical case, where an agent moves into an observer's line of sight. That should lead to a spotting event, just like moving the vision sensor.
And the computations necessary to trigger the event are just the same.

I suppose it's fair to say that *vision is a brain problem, not a physics problem*.
So handling it in the logic world isn't so crazy as all that.
And so raycasting isn't a physics responsibility at all.

How to trigger events when an object swims into view is still an interesting problem, however.

* We could start every update cycle with a vision probe along the current 
  look angle. But if the object we are seeing hasn't moved, then it seems wrong to generate a new event in this case.
* Maybe this is something that represents too much realism, and we should 
  just ignore it?
* If it really matters, then we could pass in an "objects that have moved" 
  list from the physics update, and filter by that.

I think I vote for the second alternative: do vision probes only on either setting or reading a vision register.
Note however that JSRoboWar does do radar and range interrupt checks as part of its chronon-initial interrupt checks.

## Initialization and Object Creation

Player agents are created on game startup.
It is conceivable that, once we have enemy agents, they may spawn during the game.
Projectiles are spawned all the time.
I don't know about blows: they might not be independent objects? Have to think about those some more.

On startup, we have a list of player agents to create, from some sort of configuration data.
Creating an agent means creating it in all three worlds:

* Add it to the physics world (and get back its body ID)
* Add it to the logic world (and get back its model ID)
* Add an initial sprite to the graphics world (and get back an ID)

In the case of the graphics world, you get back the ID of something abstract that handles all the sprites for that particular object.
Like a *SpriteManager* or something like that.
*MetaSprite* maybe. *SpriteMaster*.

Then you have to make sure everybody knows all the other IDs.

For example, the renderer gets a list of position updates that is generated by the physics engine. 
Each record says that "[Body ID] is now at (X,Y)".
It is up to the renderer to figure out which sprite master corresponds to that body ID.
At some future point we might even want to add decorations to emphsize collisions, so the renderer would want the collision list as well.

### Removal, Again

So if a projectile hits a wall, and we want to make it look cool,
how does the renderer get the position of the projectile?
In theory, it has already been removed from the game.
The sooner the better, so we don't waste time having it show up on radar, for example.

I don't think the collision report says exactly what point is in collision.
Anyway, it might look odd, since the collision has been resolved by the time anyone gets a look at it. So the colliding bodies have "moved on".

> Though really projectiles don't hardly bounce.

I hope that's not really true, though, since we are counting on the projectile position to tell us point of impact. I guess there's a parameter you can set that will make something totally not bounce. Restitution, I think.

Maybe the physics wrapper can maintain its own list of bodies?
That is, a map from IDs to bodies, or `null`, in case the body has been removed?
We might need such a list anyway, since the physics engine might not want to expose an index like that. 
There may be no other way to look up a body by its ID.

Maybe we can do something like the following:

1. Physics detects the collision of a projectile with something.
   It adds a collision record to the log.
2. Logic determines that the projectile should now be removed from the game,
   in the collision handler for projectiles. It also adds a "set sprite to exit animation" to the graphics updates.
3. Graphics, as usual, gets the current X,Y and orientation of each object, 
   including the projectile, and switches to the end-of-life animation.
4. Physics (next time around the loop) starts its update by looping through
   the *to-remove* list, and removing objects from the world, before calling
   update on the actual physics engine.

So the point is that we do not actually remove anything until we have had a chance to set its consequences in motion. So we can store anything we will need later.

So, when do we remove the model?
Note that projectiles do have models, even though `update` is trivial for them.
We still have to know the projectile type and its payload of energy.

Also, it is possible that some weapons might have long-term effects. 
Bombs might do damage over the course of several turns, even after the physical bomb is no more.
I guess we go from being an object of type *bomb* to being an object of type *blast*.

I guess we could do the same *to-remove list* trick with the logic world too.
Though really it shouldn't be needed. It's exactly in the logic world that we discover that something should be removed, so we should be able to do it locally.


### ID Maps and Update Logs

Suppose the physics wrapper can actually manage to get a list of current positions, to feed the logic module.
That list is going to be in terms of Body IDs.
Theoretically, somebody somewhere knows that Body 1 corresponds to Model 3, since they were created together.
But that means an extra `map` step on the updates list.

Another problem with the log approach is that we may be recording things that no one needs to know, thereby doing extra work. 
And extra work is one thing we really want to avoid.

One very important thing about the update logs: 

> All information that anyone needs to know outside the emitting world is 
  contained in the log record.

So there should be no cross-world querying.

Is this really feasible? Consider raycasting for vision.
Well, the point is that X,Y coordinates are part of the model.
That is, we keep local copies, so that we can do exactly that sort of thing.

There's also an efficiency issue.
If the physics update creates a list of all the positions of everything in the game, every frame, then we are really close to making a full copy of the game world every time through, instead of just having a single copy that everyone can get at, but mostly only with read-only access.

Unfortunately, there's a kind of paradoxical situation here.
We can't have a single *model of record*, because the physics engine needs to do its own thing with its own data structures.
Even if we could make our own model data piggy-back off of the physics body objects, we would run into trouble when we wanted to remove an object from the simulation world.

> Actually, that might not be a problem. If someone else still has a 
  handle on the relevant body object, it will not be destroyed. Just removed from the scope of the physics update step.

There are still separation of concerns problems to solve, though. 
Even assuming a single object could wrap both the body and the model, 
and maybe even the sprite master, 
that would allow and encourage calling arbitrary methods and accessing arbitrary properties of things.

I think we should try the update log approach, and see if it's feasible under game constraints.

Now, how and when do we *clear* logs?
For example, the game logic can't *consume* the position update records, since the renderer presumably also needs them.
And we can't clear all logs at the end of every game loop,
because the logic engine will have posted some change-of-velocity updates that must be read in by the physics engine.
Also, the list of bodies to remove from the physics world.

There are certain properties of game objects that *everybody needs to know*.
Mainly these are:

* (x,y) position
* drive vector/angle (or "static")
* object type (agent, projectile, blast?, wall?, etc.)

Unfortunately, position cannot be updated by anything but the foreign physics library. Drive vector is the responsibility of the logic engine.
Probably really only the logic engine cares about type, 
but there may be some way to filter physics queries and the like according to user defined object classes, maybe?

So really everything we need to know is contained in the model, 
with the single caveat that the model must just echo the physics world's position data.

That means that, in order to render a game object, we need to have a reference to its model.

It's looking more and more like what we really have is a standard tree of game objects. The physics body component has two owners (the engine and the game object), but for our purposes the object is the owner of record, and the physics world just borrows its reference.

## Game Objects and Encapsulation

So for now a game object has the following parts:

* A physics body
* A model
* A sprite master

And the main Game Object includes:

* A physics engine
* An inferencer
* A renderer
* A hierarchical collection of Game Objects

The physics engine only cares about the object's body,
and the renderer only cares about its sprite master (and its position!).

The inference engine must tell the physics engine where our engines want to drive us, and it must tell the renderer when to change animations.

To a considerable degree, the "inference engine" isn't really a separate engine. 
It is just the implementation of `update` for sentient game objects (aka *agents*).
So game objects aren't just data. They come with behaviors as well.

So back to our classic problems.

### Collisions

Periodically, during the physics update, we will be notified of collisions.
We will be told which *bodies* were involved, but we will not automatically know which *game objects* were.
So we need an efficient way to get from a body object to a master game object.
One obvious choice would be a reference, but that creates a reference cycle, and doesn't that cause problems with garbage collection?

> No, apparently it does not. Modern garbage collectors do not use reference
  counting, and they can recognize these kinds of cycles. If nothing else refers to the objects in the cycle, they will be garbage collected.

So we actually can add a `body.gameObj` property. We don't have to use indexes or IDs.

### Death and Destruction

The update discovers that our agent now has zero health, or whatever other condition would warrant it's removal from the physical plane (e.g., it hit zero health *n* turns ago).
Removal from lists is really problematical if we use array indices as references anywhere.
So we have to be really careful about that.
But if we use references for references, maybe not so bad.

So, `PlayerAgent.update`:
```
if (this.model.health <= 0) {
	game.physics.removeBody(this.body);
	this.model.isActive = false;
	this.graphics.setAnimation('dying');
}
```

### Drawing Layers

Not only does the renderer have to know which frame of which animation to draw, where, and rotated in which direction, 
it also has to know which drawing layer to render it on.
Currently this is done by looping through all the objects once for every layer, and skipping over objects that don't draw to the current layer.

It would be nice if we could just move the sprite master from one layer to another. That is, if layers were collections of sprites.
But with a not-dead agent, we have to draw to two layers.

The correct way to do it is probably to have multiple canvases.
I think there is a `z` property that specifies rendering order.

Otherwise the only thing I can think of is separating the generation of objects to draw from the actual drawing.
So first `render` creates a list of drawing instructions, really three lists that get concatenated together, and then it maps over the list and executes them.

That looks suspiciously like three loops, but note that the lists will be quite a lot shorter than the whole list of game objects.
And they will be generated in a single pass over the object list.
So "drawing to layer 1" means "add drawing instructions to sub-list 1".

What is a "drawing instruction" in this case?
For a player agent, there's a few separate things that might get drawn:

* The player body
* The aim indicator
* The health bar
* The label

If the agent is dead, then the health bar doesn't get drawn, and everything else gets colored gray. 

In JavaScript, it is possible to call `apply` on a function object, with an array of arguments.
So a drawing instruction could literally be something like:
```
{
	op: ns.drawHealthBar,
	args: [x, y, percent]
}
```
And rendering is then just:
```
instr.op.apply(instr.args);
```
Emitting the instruction could be something like:
```
preRender: (layers) => {
	if (this.model.isActive) {
		let pos = this.getPosition();
		layers[Layer.ACTIVE].push({
			op: this.renderBody,
			args: [pos.x, pos.y],
		});
		layers[Layer.LABELS].push({
			op: this.renderHealthBar,
			args: [pos.x, pos.y, this.getHealthPercent()],
		});
		layers[Layer.LABELS].push({
			op: this.renderName,
			args: [pos.x, pos.y, this.name],
		});
	}
}
```
As an optimization, note that any drawing to the bottom layer can be done immediately.
The key goal is to delay the execution of drawing instructions directed at the higher layers.
So then the top level looks like this:
```
layers = [[], [], []];
objects.forEach((obj) => obj.render(layers));
for (var i = 0; i < 3; ++i) {
	layers[i].forEach((i) => i.op.apply(i.args));
}
```
Probably we want a separate `createLayers` function to make sure we get something that is compatible with the layers we have defined.


# 2017-09-06

## Events

If we communicate using events, then every event type has to have a set of listeners, and every event of that type has to be published (somehow) to all its listeners, and then removed.

It would be nice to have game data be global readable,
but if so we have to control writing somehow.
Maybe writing is only possible for game data event handlers,
so whoever's in charge of the game data component gets to control when things get written. 


## The App

So what happens when you first navigate to a MAGIC site? 
What do you see? What can you do?

The main thing you can do is to issue the command "New Game".
But there may be some other things available:

* Replay from log
* Upload player agents
* Select rules/engines/game type
* Upload/Select a map
* Configure (and run) a tournament

Maybe some other stuff.

So the main thing we have to do on startup is to enable the UI to receive these various commands. 
That may not be very interesting. 
There might just be a big shiny button in the middle of the screen labeled "New Game!". 

In that case, everything starts with that button's `onclick` handler.
Which is essentially our current `main` function.
Only for now it's rigged to start automatically on window load.

But maybe we really should call it `newGame`, not `main`.

By the time you hit the New Game button, 
it should be clear what sort of map and arena/viewport should be presented to the user.
I guess there will also be some *transport controls*. Though I doubt "Rewind" will be one of them! Just "Play" and "Pause", really, with pause being equivalent to stop.

In the initial case, the map is just four walls around a 800x600 arena.
The viewport takes in the entire scene, so we can ignore it for now.
Also, the roster has been pre-selected.

So the main thing we want to do is to construct a Game object, 
and tell it to `start()`.

## Game Objects

Let's start with *walls*.
Each wall is a game object. 
It has a body that participates in collisions.
It has a sprite master that knows how to render it.
It doesn't have much logic, though, and probably doesn't participate much in the logic update cycle.

> That could change if walls can take damage and be destroyed.

The assumption is that, in the simplest case, every call to `update` and `render` branches recursively to all the game objects.
And each game object has `update` and `render` methods.

Note that the physics engine has its own internal loop, so the master update method just calls `physics.update()` and that's the end of it.

Note that JSRoboWar splits game objects in two, so we have for example Robot and RobotView, or Arena and ArenaView.

But here it looks like we want something like
```
class GameObject {
	constructor () {
		this.body = {}
		this.model = {}
		this.sprite = {}
	}
	update() {}
	render() {}
};
```

Then I guess they get added to collections attached directly to the top level Game object.

This does leave us with some inefficiencies.
For `update` and `render` we end up looping over the entire collection of game objects. It is conceivable that there could be things to draw that have no logic or physics, and vice versa.

This might get critical later on when maps can be bigger than our viewport.
Then maybe we really don't want to render stuff that is offscreen.

If there is a collision with a wall, will the inferencer need to know anything about the wall? 

How do walls even draw themselves? How do they get their X and Y coordinates?
Do they have to call something like:
```
this.game.logic.find('NORTH').getPosition();
```
That would be awful. It requires knowing so many APIs that you have to pass through.

Another problem with position is that it's sometimes in different coordinate systems. Like Matter uses body centers for position, but for drawing a rectangle we'd most like to know its top left corner (min x, min y).


### Game Objects and Collisions

If there is a collision, then the inferencer will get a collision event record.
If it's just straight out of Matter.js, then it just mentions the body objects that collided.

The physics wrapper should translate raw Matter collisions into collision reports written in terms of game objects.


### Drawing Objects

Just take as an example the four walls of the arena.
The "map", in other words.

A map is a composite object consisting of various wall segments.
In our case it's super simple: there's four of them.
They are rectangles, with a particular position.
Their size is based on the global arena size, and a global constant wall thickness value.
Their color ("texture") is known only to the renderer.

When we create each wall, we have to create a physics body and a drawable
with code to draw a rectangle in the `GROUND` layer.

```
addNorthWall(map) {
	let width = map.outerWidth;
	let thickness = this.game.const.WALL_THICKNESS;
	let northBody = Physics.wallSegment(
		width/2, thickness/2, width, thickness,
		{ isStatic: true, label: 'NORTH' }
	);
	let northView = ns.Graphics.rectangle(
		0, 0, width, thickness);
	map.addBody(northBody);
	map.addView(northView);
}
```
Maybe:
```
addWall(map, x, y, w, h, name) {
	let body = Physics.wallSegment(
		x + w/2, y + h/2, w, h,
		{ isStatic: true, label: name });
	let view = Graphics.rectangle(x, y, w, h, name);
	map.addBody(body);
	map.addView(view);
} 
```
So it looks like `map` might have a body, and also a list of sub-bodies, and a view, and also a list of sub-views.
This does not make me happy.
But if you want to remove a map from the game entirely (or some other composite), then you will need to be able to find all those other parts.

Really, a map should have a body which is a composite, and a view which is also a composite.
I don't know how this would work for physics, though.
Maybe a map isn't really a thing.
It's just instructions for adding some things, that otherwise have an independent existence.


# 2017-09-09

## Rendering Sprites

In the current state of development, "sprites" are not sequences of bitmaps from a spritesheet. We actually follow a sequence of drawing instructions.

So the "sprite" is actually a function.
And when the actor dies, that function is replaced by a different one.

So instead of having a keyed list of static assets,
our crude sprites have (I guess) a keyed list of functions.

There's two of them, corresponding to states of the controller,
so we can call them `DEAD` and `NOT_DEAD`, I suppose.
They each need to know, as arguments:

* Sprite position
* Sprite aim
* Base color

Well, only `NOT_DEAD` actually needs to know the color.

You know, I'm having my doubts here.
Having all of that stuff together as the properties of Actor States
was really handy.

If we really want to control what gets drawn, to save looping over all objects, then we can have a separate list of *visibles*, which would just be borrowed references to the actual objects controlled by the main game logic.

Likewise a separate list of *updatables*, to control the length of the list we map `update` over. 

But those are optimizations that would be premature at this point.

Also, I think distinguishing Game from GameLogic isn't getting us anywhere.

Having a Graphics object around is still useful, since it provides a custom drawing context (LayeredCanvas), and potentially also a wrapper around whatever graphics library we might eventually use.

But what about when we do have a whole spritesheet to manage?
Including which particular image we are currently using, and other animation data?
It will be very convenient to have a separate object that can deal with that.
The only real change here is that that object will be a member of a master Game Object.

So `render` is called on the game object, and it delegates to its sprite master. But in the meantime we can pick up some data from the model to pass down to the sprite master, so it doesn't have to have an internal reference to the model.

The big thing is that, without a separate graphics library,
we have to add the physics body to the physics engine, 
but we don't have to do anything special with the sprite master.

### Handling State Changes

The game knows only game objects.
So when it calls `update` or `render`, it calls them on the game object.
Game objects know about states, and sprites.

So, when a call to `Actor.update` causes a state transition,
from `NOT_DEAD` to `DEAD`, or from `DEAD` to `ELIMINATED`,
this can in turn change the current sprite.

In general, `update` is always able to change the current sprite,
by requesting a new animation.
So the sprite has to always be controlling a collection of animations that it can switch between.

> Generically, an "animation" is just an object that actually knows how 
  to render its controlling object. No delegating, no buck-passing. 
  Just actual graphics world drawing instructions.

It follows that all drawing instructions should be located in Sprite classes of some kind.

In a future world, we may be able to construct a sophisticated "just in time" resource manager, so that the sprite can load resources only when they are needed, and can cache old resources that are likely to be reused.
But for now, a sprite on construction will have a full list of all the separate animations that it might ever need.

So, on state change, a game object has to be able to tell its sprite master to change animations. I guess by passing it some kind of shared key.

That means in turn that the architecture of our previous State classes changes.
The result still seems kind of sloppy to me.

```
PlayerAgent.prototype.update = function () {
	let result = this.state.update();
	while (result === Q_DONE) {
		this.state = this.transition(this.state.id);
		result = this.state.update();
	}
}

PlayerAgent.prototype.transition = function (q1) {
	switch (q1) {
		case Q_NOT_DEAD:
			this.sprite.select(DEAD); // <---
			// And request removal from the physics world...
			return new StateDead(this);
		case Q_DEAD:
			this.sprite.select(ELIMINATED);  // <---
			return new StateEliminated(this);
		default:
			throw `No transition out of state #${this.id}`;
	}
}
```

Now suppose that we have a special sprite for "walking north east".
Is it the responsibility of the model to know that (a) we are now headed close enough to NE, and (b) there is a sprite for that?
It seems like this should be part of the *render logic*.
And the render logic should be encapsulated in the sprite.

So maybe it is the sprite's responsibility to know that it has a "dead" animation, as well as a "not-dead" animation.
And so in `render` we query the model's state ID, and branch based on that.
(It could be any aspect of model data, like "health is 0", or "heading is NE-ish".)

Now the fact that there is special "I'm dead" *logic* and "I'm dead" *graphics* is completely accidental. The fact that they both change at the same time is not special in any way.
And that kind of makes sense, given that there is no special logic for walking northeast.
It's up to the sprite designer to know what resources they have, and how to deploy them.

Then the question becomes how intimate can the sprite get with the model?
How much access to model properties does it get?
I guess the short answer is "Methods only!", no direct member access.
But that's pretty weak: we can always make a new method.
But at least that gives a little separation, allowing field names and structures to change in the model, without changing the API.

So that means that we don't need separate state classes at all.
Rather, we need an `update` method that can branch to `notDeadUpdate`, `deadUpdate`, and `eliminatedUpdate`.
