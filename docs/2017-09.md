# 2017-09-01

## Handling Dead Player Agents

I'd really like to move them to a separate list.
Maybe even separate object types.

The expectation is that they will be rendered differently.
For example, the agent sprite might be replaced with a brief explosion animation.

If corpses are displayed, they should be on a layer below where the player agents are drawn. That would give us *three levels* now:

1. Corpses
2. Player agents
3. Player agent decorations

Then we handle death by constructing a new DeadAgent object with the same name as the former PlayerAgent. It might have the same sprite, even.
For now, since there are no sprites, we just want to draw a gray circle, the name of the bot, and its aim direction, mainly as debugging aids.
Then maybe implement animation by just having the "sprite" fade away.

Note that this means we probably want to draw the name on the bottom layer, 
as well as the disc.

So continuing with the death handler:

1. Construct a new DeadAgent object from the dead PlayerAgent.
2. Remove the agent's body from the physics world.
3. Remove the agent sprite from the sprite list.
4. Add the dead agent to the dead agents list.
5. Render dead agents, sprite discs, and sprite decorations, in that order.

Someday we might want exploding bots to be a hazard to other bots, in which case there will be a time when the dead bot is still involved in collisions.

The big question is whether we really want a separate dead bot class, 
or rather a single bot class that can be in multiple states.
I am now thinking the *state machine* approach is probably a better one.

Currently there's only two states an agent can be in: `Alive` and `Dead`.
But later there could be lots more. 
Even the `Dead` state might want to be split into `Dead1`...`Dead8` or whatever, to handle gradual elimination from the game. For example, if we want the appearance to change gradually, or we want explosion damage to decrease over time.

That leads to the question of how agent state interacts with animations.
A *state* might actually be a complex object capable of internal transitions.
In that case, it needs a final transition to another meta-state. 
For example, what does `Dead.8` transition into?

There can only be one runner responsible for state transitions.
It queries the state for its next transition, given environmental data (e.g., a reference back to the controller). The state may respond "Done!", in which case what would we do next?

There has to be a top level transition table that says "Enter meta-state `Dead`; on `Dead.done`, enter meta-state `Eliminated`," or something like that.

The idea is that on entering state `Dead`, there is an assignment like:
```
sprite.stateData = {counter: 0};
```
And the controller enters a new state by recursively calling something like 
`stateData.next()`.

This means we are running an RTN, but hopefully one with a very strictly bounded stack.

So stuff happens to an agent according to its current state.
Then after all that, we check to see if its state changes.
Then more stuff happens according to the updated state.
Both `update` and `render` would be sensitive to the agent's state.

There has to be a uniform state API.
Currently that is just a single `next()` method.

The main problem is that if there is just a single `state` property that we plug complex sub-states into, they don't know what to transition to when they are done. 
There really has to be a *stack* of states. So the top-level `next()` can call the embedded `next()`, and if it returns "Done!", then the top-level transition table says what to replace the sub-state with.

So there's `PlayerAgent.prototype.next`, and also `PlayerAgent.prototype.state`, which is inhabited by an object that also has a `next` method.
```
next = () => {
	let rslt = this.state.next();
	while (rslt === Q_DONE) {
		this.state = State.create(this.delta(this.state.id));
		this.state.next();
	}
}
```
So state class `Alive` would only return `Q_DONE` when the agent's health dropped to zero or less. Then it would transition to `Dead`.
And state class `Dead` would only return `Q_DONE` after an internal counter had reached a predefined limit. Then it would transition to `Eliminated`.
State `Eliminated` would trigger cleanup of the agent.
Meaning some top-level game logic in `update` would have to ask "if X is in state Eliminated, remove it from the list, or always skip it in update and render, or whatever."

The state API might need to include its own `update` and `render` methods as well.
I guess `update` is really where it would make its own internal transitions.

Changes in health will happen in *collision handlers* (mainly when agents collide with weapons and ordnance). These will be called by the physics engine during its `update` step. And that, in turn, happens at the very end of the main `update` step.
Taken all together, that means that an agent will actually die in between the end of the update step and the beginning of the next render step.
That doesn't seem right...

If you die during the update step, then we want you to appear dead in the immediately following render step.
To make that happen, we would have to do the physics step, then the sprite step.
So the new order is:

1. Advance physics simulation
2. Bot update: execute program for 1 chronon (if in state Alive)
3. Render

It's easy to imagine other states, especially as ways of implementing debuffs.
Being stunned or slept would be just like being dead, but on `Q_DONE` you would transition back to `Alive`.
But for right now we just have two states: Dead and NotDead.

### StateDead

I guess the only property of a dead agent is a counter to say when to clean up.
So more or less:
```
next: () => {
	if (--counter === 0) {
		return Q_DONE;
	}
	return Q_NOT_DONE;
}
```

In the long term, StateDead might come with its own sprite animation.
And it certain comes with a unique state id (`Q_DEAD`, presumably).

Oh, and I guess it needs a `position` property, since that's no longer available from the physics body. Though in fact there is no longer any reason to destroy the physics body, so maybe that's not a problem going forward.

### StateNotDead

This would have a `Matter.Body` member, and a `health` property, at the very least. Currently we also have stuff like `aim` and position and motor vectors, which are really redundant with the body data.
There will be lots more later, corresponding to various agent "loadouts".
More importantly, there will be a `program` property.

An update step for a not-dead agent will involve:

1. Checking state and transitioning if necessary
1. Running the program for `cpuSpeed` cycles
2. Updating the physics body properties accordingly

It may be that the program is allowed to update physics properties directly.

Ultimately there may be a two-step process where running the program just creates a log of side-effects, and a second stage actually evaluates the log and changes the world accordingly.

At one point I was thinking of running all active programs in parallel.
That would not be possible if agents are updated one after another.
We would have to split the master `update` function to loop through the sprite list three times:
```
update: () => {
	for (var i=0; i< n; ++i) {
		sprites[i].updateState();
	}
	interpreter.advance(sprites);
	for (var i=0; i< n; ++i) {
		sprites[i].playLog();
	}
}
```
Something like that.
Is there still any reason to do this?

No sprite can move during a chronon. 
So even though you can move your turret, it will only register sprites in the state they were in before the update.
You can fire, but your attack will only be launched at the end of the chronon.
That is, a new ordnance sprite will be created and added to the projectiles list and the physics world, but it won't start moving until the next physics update.
So all projectiles will appear to happen simultaneously, I think.

Again, all events that can damage you happen during the physics simulation step.
If we need to refine things, then we need to have several short-step/logic iterations in between frame rendering.

### StateGarbage

After you've been dead for a little while, you become garbage, which needs to be made to disappear. 
Either we actually remove all sprite data from the game, 
or we leave it like this and make sure our loops skip over all garbage sprites.
They are completely invisible and undetectable, and do no work of any sort. 
They just take up memory.
And this is a state that never emits `Q_DONE`. It's like a sink state in automata theory.


## Programming Language

I notice that JVM bytecode is a stack based language, like RoboTalk.
It doesn't look like there's a lot of `DUP`, `ROLL`, `DROP` instructions, though. Some of them exist, but don't seem to be used a lot.
Actually, on closer reading, there's a lot of finicky `dup` instructions, so I guess it isn't enough to drive people away from stack machine programming.

The key feature is *event handlers*, aka *interrupts*.

Suppose we have an event handler for colliding with a wall.
That's code that exists in the program, but the event itself happens during physics simulation.
That means that the actual event handler, the one registered with `Matter.Events`, is going to just add an event to a queue or put a message in a bot's mailbox, or something like that.

Now, it's possible to be in collision with more than one thing at once.
In a corner, you are hitting two walls. You may bump more than one other player agent. And so on.
So there's going to be multiple events, even of the same type.
Do you handle them sequentially? Only handle the first one?

First off, different events may have different *priorities*.
So on returning from your priority 1 event handler, you may immediately get bounced to your priority 2 event handler.

There should be an option to drop the remainder of the event queue.

Maybe the way to implement a collision handler is to get a list of *all* the objects you are in collision with.
That's the only reliable way to compute your best escape vector, for example.

In a stack machine, you'd have to do something like have the array length at the top of the stack.
And you'd have to have instructions for pushing the item at *stack - N*.
Also, returning from the handler would have to unwind the stack entirely.
So the arguments form a sort of *stack frame*, I guess kind of like in the JVM?

The actual items on the stack would have to be some kind of *object references*, where the objects would live somewhere else. 
Note that now we need some sort of *garbage collection*.
In this particular case, since the objects are created by the game engine, and are not needed after the event has been handled, they could actually be included in the stack frame, or automatically destroyed as soon as their reference goes out of scope.

Note that if `goto` is allowed from inside an interrupt handler to outside it, then we can be in very serious trouble.
We'd have to be able to analyze all `goto` targets and determine which ones would cause an exit from the current block.

### Collisions

Are collisions the only game events that get reported by the physics engine?
So far, I think that is the case.

The engine gives us a list of affected pairs. 
In addition, there are two distinct events that we may want to collapse.

1. `collisionActive`
2. `collisionStart`

So we have to scan the lists of affected pairs for both types, 
and deliver the relevant records to each affected object's event queue.
Ideally they would be ordered by *timestamp* as well,
but for now we may as well consider events occurring in the same engine step to have been simultaneous.

Maybe we can just react to `collisionActive` events?

The information we want to deliver with each collision:

1. Who/what we are colliding with.
2. Direction of the collision?
3. Force of the collision?

Not exactly sure how we calculate these.
Really, the most important direction is the direction to the other sprite, and we get that from the sprite itself.

So there has to be a way to register a handler for a given event.
The VM will come with a set of fixed memory locations that will hold code addresses. So something like:
```
STORE collision, OnBump
```
Where `OnBump` must be the label of an address.
It should also be possible to *compute addresses* and store the result of the computation.

### Memory Model

So, apart from the stack, there will be a collection of fixed memory locations, AKA *registers*, where we can store stuff.
Among these would be the *interrupt handlers*, 
and *hardware registers* like `aim`.
There also needs to be some sort of user memory.
I guess any symbol that isn't a register name or an interrupt name could be interpreted as a memory location.

It should also be possible to declare a constant data area.
That is, the user should be able to declare named constants, for code readability.

I guess we can always use tagged symbols.
Like `@foo` is the address of label `foo`; `@(foo + 6)` is the address 6 after `foo`, etc.
And maybe `!collision` is an interrupt handler register,
and `&aim` is a register, I dunno.

In RW it was necessary to distinguish literal register names from the value stored there, so you could store a register name in another register.
Maybe that will be necessary here as well?


# 2017-09-02

## Programming Language

The main goal is to have a language that makes it clear how many "CPU cycles" a given program fragment is going to consume.
So the simplicity of assembler, bytecode, and three-address code is appealing.
But we do not require the efficiency,
nor do we need to worry too much about how many real cycles our interpreter is taking up per instruction. 

For example, there's a lot of work going on in a function call: stack frame allocation, pushing parameters, saving the return address, and so on.
And compiling an access via a path through a struct leaves us with something quite simple at run-time, but if any part of that path involves de-referencing a pointer, then suddenly there's more instructions.
Maybe we don't care so much, here?

Also, in TAC, we'd have to compile out an array access into a number of steps, possibly. But we can just say "`[]` is an operator, like any other".

### Are registers just storage locations?

I've written register storage as two steps:
```
* 		driveX 	-1 		_T4
Store 	driveX 	_T4 	_
```
This is because, in the back of my head, I know that storing to the drive register is a method call, not simply an assignment. 
But that's under the hood. How complicated would the interpreter become if we allowed direct storage in TAC?
```
*	driveX	-1	driveX
```


## Garbage Collection

Note that we do not want to just transition bullets to state `Eliminated` and then leave them around and just skip over them. 
Memory will leak away really quickly this way.
Bullets definitely want to be deallocated once they have died.


## Classes and Modules

We're getting to a point where the PlayerAgent code wants to be split off in its own separate module.
What other modules do we want?


## States

If states handle their own rendering, then the state API includes both `update` and `render`. 
At that point, it starts to look a lot like `PlayerAgent` itself...
```
update = () => {
	let rslt = this.state.update();
	while (rslt === Q_DONE) {
		this.state = State.create(this.delta(this.state.id));
		this.state.update();
	}
}
```
I guess for `update` there's still the question of who handles top-level transitions. 
But for example the `StateNotDead` object is now the thing that actually runs the interpreter.

I guess to some degree it's the old question of whether to implement variations in behavior by inheritance or aggregation.

Anyhow, the point is that now top-level `render` is invariably trivial:
```
render = () => {
	this.state.render();
}
```

Perhaps it is still the case that the player agent *model data* is all still held in the main PlayerAgent object.
And so the methods that manipulate it (getters and setters and their ilk) also belong to the main object.
It's really just `update` and `render` that get delegated.

What about top-level `delta`?
```
delta = (id) => {
	switch (id) {
		case State.Q_NOT_DEAD:
			return State.Q_DEAD;
		case State.Q_DEAD:
			return State.Q_ELIMINATED;
		case _:
			// Eh? Nothing else should ever be Q_DONE!
			break;
	}
}
```
I guess that's more or less what it would look like.

I guess initial state is always `StateNotDead`, though I suppose it could someday be a special `StateInitializing` or something like that, for one time start up processing.


# 2017-09-03

## Vision and Aiming

I was thinking of just using the `aim` angle for both vision and shooting.
But it might be better to keep them separate.
For example, if you see an enemy at point V1, but you want to lead your shot, then you have to look away, maybe messing up your tracking routine.

> In other words, it's important to be able to do shot-leading and 
  object tracking at the same time.

And that, in turn, means that their memory systems have to be distinct.
A different set of registers.

Right now, I am not interested in shooting, just vision.
Maybe call it the `look` angle, not `aim`.

Also, the vision system ought to be capable of replacement.
For bots, it can be a single ray, like in robowar.
But for humanoid dungeon explorers, it will want to be a cone, or a fan of cones, or something.

Note that Matter's ray-intersection query is supposedly implemented by doing collision detection with a very thin rectangle. 
If that's so, then really there's no deep difference with using collision detection with a triangle or pie slice.

Next question: Can bullets prevent you from seeing an enemy?
I kind of think not.
So, like robowar, we might distinguish routines that look for enemies from routines that look for bullets.

### Implementation

I guess I want to trigger an interrupt every time `look` changes to a new angle, and there is an enemy visible along that ray.

So the code for `setLook` has to test to see if the agent has an interrupt set for the `look` register, and branch to it if a `Matter.Query.ray` test comes back positive. 

Also the collisions returned by Matter have to be reduced to a single nearest object.

Someday there may be an interrupt parameter that limits the distance.

Also and in any case the distance is an important thing to know, and maybe our hardware can work that out for us.

For now, we expect the interrupt handler to be a JavaScript function object living at `agent.state.prototype.on<Event>`.
So `setLook` would check to see if that property exists and is defined, and, if so, it would call it, with the sprite that triggered the interrupt.

The RW design of the aim register and friends has them be readable as well as writable. 

* Writing changes the angle
* Reading returns the distance to the nearest thing, or zero (I think)

So you can say:
```
let o = getAim();
if (o) {
	// fire!
}
```
But now how do you read the look angle?
Like if you want to set `aim = look`?
Maybe make the register complex?

* `look.angle`: writable and readable
* `look.thing`: read-only

It would be nice if `getLook` returned `look.thing`, so you could still write:
```
if (getLook()) {
	// ...
}
```

# 2017-09-04

## Vision

One problem with `Matter.Query.ray` is that it needs a collection of bodies to search among. 
And that, in turn, entails access to the physics engine.
But if `setLook` is a method of PlayerAgent, then we have a circular dependency, because the main code needs PlayerAgent to have been loaded, and now PlayerAgent needs the main code to have been loaded...

Maybe factor out the physics stuff? Is that possible?
It is possible, but it means that `matterEngine` is now a public member of the `MAGIC` namespace.

But we still need some way to get the list of agent bodies from the engine.
This list is only populated in the main routine. 

Maybe I need to be thinking less in terms of objects with accessor methods, and more in terms of global functions. 
So instead of `agent.setLook(...)`,
use `MAGIC.setLook(agent, ...)`.

It looks like some things have to be declared early, 
before they are populated.
So more than just the physics interface needs to be extracted.


## Game Data

Maybe we need to define a general GameData object before we populate it.
It could contain the `arena` and the `sprites` list,
and eventually also the `projectiles` list, 
and the map.

### Agents

Should this be a list, or something indexed?
E.g., by agent name.


## Aside on Quadrants

If our aim is close to 90 degrees, then maybe the (+,+) quadrant is not what
we want. Maybe try a *diagonal* quadrant system. 
Which quadrant system you use depends on which mutliple of 45 degrees the ray angle is closest to.

( 45, 135)	y > x && y > -x  <==>  y > abs(x)
(135, 225)	y < -x && -y < -x  <==>  abs(y) < -x
(225, 315)	y < -x && y < x  <==>  y < abs(x)
(315, 45)	y < x && y > -x 
			-y > -x && y > -x  <==>  abs(y) > -x

## Ray Casting

Looks like Matter's ray query is flukey. 
It is still turning up sightings of dead bodies...


# 2017-09-05

## Ray Casting (Cont'd)

Found a really nice looking algorithm for line-circle intersection, which even deals with the tricky case of line segments.
It looks like it only takes a single square root, so it should be pretty efficient.
Stack Overflow URL:
```
https://stackoverflow.com/questions/1073336/circle-line-segment-collision-detection-algorithm
```
Parameters:
```
Taking

    E is the starting point of the ray,
    L is the end point of the ray,
    C is the center of sphere you're testing against
    r is the radius of that sphere

Compute:
d = L - E ( Direction vector of ray, from start to end )
f = E - C ( Vector from center sphere to ray start ) 
```
And the code:
```
float a = d.Dot( d ) ;
float b = 2*f.Dot( d ) ;
float c = f.Dot( f ) - r*r ;

float discriminant = b*b-4*a*c;
if( discriminant < 0 )
{
  // no intersection
}
else
{
  // ray didn't totally miss sphere,
  // so there is a solution to
  // the equation.

  discriminant = sqrt( discriminant );

  // either solution may be on or off the ray so need to test both
  // t1 is always the smaller value, because BOTH discriminant and
  // a are nonnegative.
  float t1 = (-b - discriminant)/(2*a);
  float t2 = (-b + discriminant)/(2*a);

  // 3x HIT cases:
  //          -o->             --|-->  |            |  --|->
  // Impale(t1 hit,t2 hit), Poke(t1 hit,t2>1), ExitWound(t1<0, t2 hit), 

  // 3x MISS cases:
  //       ->  o                     o ->              | -> |
  // FallShort (t1>1,t2>1), Past (t1<0,t2<0), CompletelyInside(t1<0, t2>1)

  if( t1 >= 0 && t1 <= 1 )
  {
    // t1 is the intersection, and it's closer than t2
    // (since t1 uses -b - discriminant)
    // Impale, Poke
    return true ;
  }

  // here t1 didn't intersect so we are either started
  // inside the sphere or completely past it
  if( t2 >= 0 && t2 <= 1 )
  {
    // ExitWound
    return true ;
  }

  // no intn: FallShort, Past, CompletelyInside
  return false ;
}
```

There might be simpler solutions if I just need a yes/no answer, but if I need the actual intersection points (e.g., for doing my own collition resolution), then this looks pretty good. 

It does not return the distance, I think: that would cost another square root. 
But if we already know the view length, then that gets to be part of the *L* calculation. So a maximum distance is built in (at the cost of computing *L*, which is simple but not easy).

### An Alternative Simple Predicate

Suppose that we think of the vision system not as being a ray, but a rectangle.
Take the diameter perpendicular to the aim vector, and the maximum vision distance, and form a rectangle. If the center of any agent is inside that rectangle, then that agent is visible.

That doesn't necessarily help us figure out which one is *closest*, but it does tell us which small subset we have to sort through.

This assumes that all agents are the same size.

Also, we have to find two points on the observer agent's circumference, 
so that's a bunch of trig calls, I think, before you even have your rectangle.



## Code Organization

This still seems to me to be a little wonky.

One thing in JSRoboWar that I liked was treating even such simple robots as composites, for drawing purposes. Draw the body, then draw the turret.
Also, though I think this was forced by the drawing library, using `translate` (for the body) and `rotate` (for the turret), rather than doing the dirty calculations locally.

Anyway, at the root of the theoretical object tree would be the *Game*.
This is really just:

* A place to dock top level game components
* A means to handle communication and coordination of top components

Among our top things would be the *GameData*, 
the visualization system (*GameView*?),
and any controls linked to the main page (start, stop, load agents).
If I'm using a physics engine, then there is also that.
That is, there is the agent, making internal decisions and activating its available controls, and then there is the world it lives in, handling its own forces (principally momentum).

But each separate game object has realizations in all of these systems.
An agent has logic that maps game data to game data,
an appearance that is handled by the game view,
and a body that is handled by the physics engine.
And they all communicate: the appearance depends on the game state of the agent, for example, and the data about its position cannot be determined without consulting the physics engine.

I think the main game loop still looks like this:
```
loop: () => {
	window.requestAnimationFrame(loop);
	physics.update();  // where am I?
	psyche.update();   // where should I go?
	appearance.update();  // what do I look like?
}
```
That's really just a variation on the standard update-render loop, 
with the additional complication of two things to update.

The calls to `update` and `render` are recursive.
That is, every game object probably has an `update` and `render` method, 
and also every object has some sort of *parent container*, leading eventually to the top level game loop.

### Aside on Models, Views, Controllers, and Events

Suppose we take the *model* to be pure data,
and the *view* to be pure drawing operations and properties.
Then one way to look at a *controller* is that it is something that responds to *events* by modifying the model. 
The view is always and only derived from the model.

Then the whole issue becomes one of "Where do these events come from?"
In a web app, events generally come from the screen, more or less, or at least they appear to. There are mouse clicks and key presses, and often we need to know a screen coordinate for where they happened.
So the "view" can *appear to be* also the "controller".

For example, if I click a button, there is a mouse click event, 
and the system has to decide what system object to hand off that event to.
The answer has to do with what object contains that event's (x,y) coordinates.
So it has a visual, or at least spatial, component.

And then that *smallest enclosing shape* becomes the object that actually now appears to be the event source.

The controller, meanwhile, is responsible for mapping event data to changes in the actual model data. 
In a way, it's like the model is a pure struct, and the controller superimposes a set of functions on top of that struct, so as to maintain consistency and propagate consequences.
So the controller is a set of event handlers which then calls these methods according to the core application logic.

Meaning that now the controller, not the model, is the locus of application logic.
I think this is a bit non-standard...

If the model of a scroll bar is something like a document line number,
then the controller would map mouse events in the various bar regions to changes in that line number. Mostly relative changes -- I think the controller would not generally need to know what the actual line number was.
And I suppose this could have knock on effects, like certainly for the document window widget, and maybe for the selection and so on.
Is that done by direct calls, or by emitting new events for other controllers to consume?

Probably the scrollable window has its own controller, because how else could consistency be maintained between the document view, selection highlight, and scroll slider be maintained?
(Consider for example the effect of a down-arrow key, which sometimes might trigger scrollbar movement.)
So events aren't necessarily just publish and subscribe, but rather things that go up the tree, and then back down. Always following ancestor-descendant paths.

So what's our analog here?
In general, there's nothing quite like a mouse-click. Events are all generated from within the game.

* The physics engine triggers *collision events*
* The turret system triggers *sighting events*

And, again, there can be whole event cascades:

* Collision with a projectile causes health to drop to zero.
* That causes a state change.
* That might introduce new projectiles (burning shrapnel),
  and new collisions.

Of course, not all in the same game cycle.

### How does animation work?

It would be nice if we could just submit animations to a separate drawing engine that would handle all that.

Meaning, it is the drawing engine that is responsible for advancing the animation frame and cycling back if it's an animation loop.
The game engine just says: "Okay, we just changed direction, so tell the renderer that we are now using animation *WalkingNorthEast*!"

That would be a huge load off of the game engine.
And it makes it easy to implement a machine instruction to switch animations (similar to RoboTalk `ICON0` and friends, only for animations).

Indeed, we could say that a *sprite* is actually an animation, a sequence of sprites, really. So have instruction `SPRITE0`, basically.

With certain sprite settings happening automatically? Like you can set direction animations to be triggered automatically on change of direction?

All this really means is that a call to `agent.render()` is going to not only draw a sprite, but also update the animation frame counter.

Note that we are probably going to have a single *walking* animation, and simply rotate the sprite in the current direction of travel.
That works for top-down perspectives.
If we move to isometric, I believe things get more difficult.
Then maybe movement becomes tile-based? Maybe with small tiles?

### Communication

For example, right now an agent has a `body` member that is a reference to its physics body, and the physics body in turn has a `spriteIndex` member so as to reach back.
We need this for things like the brain knowing what our current X and Y positions are, by asking the body.
And the body knowing what our current desired velocity is by asking the brain.

Right now this is implemented (as noted) by giving everybody direct pointers to each other, so that any method or member can be accessed and even updated.
Arrays of references (even if they are just indexes) will almost certainly lead to *dangling references* as things get destroyed and removed from the game.
Also I think *memory leaks*, as circular references can subvert garbage collection.

So, the big question is whether we have three trees, of bodies, brains, and sprites, or a single tree of objects that know how to manage all of the above.
And we know in advance that bodies and everything else are in different trees, cause that's the way the physics world works.
And if we use a rendering library as well, then probably that will take over all the sprites.
I guess that kind of answers things for us: unless I want to write a physics engine, we are going to have to have two things owning references to an object's body: the physics engine and the logic engine.

It would be nice if a recursive call to `update()` would return a list of "things to tell the physics engine," and "things to tell the renderer."
Then everything is based on ID, and the respective master object just has to handle things like IDs no longer existing in as elegant a way as possible.

Suppose the physics engine changes our position.
How do we tell the renderer?
The physics update will not cooperate in giving us a nice list of changes, at least not on its own. 
But I suppose we could engineer some sort of *wrapper*. 
That would have to query all bodies for their current X and Y coordinates, 
and perhaps also their collision status, and make a list to submit to the logic updater and the renderer.

Note that it is very important that the logic updater cannot change an agent's position. Only the physics engine can do that.
So the position input to the engine and to the renderer is the same.

The renderer does not result in any list of changes.
It is a pure subscriber to the model. 
So it's really just the physics and logic talking back and forth that we have to manage.

One interesting problem to consider is how to handle **projectile impacts**.
If a bullet hits a bot, the bullet vanishes from the physics world,
but some kind of explosion or blood spatter or something might still take a couple more animation frames to complete.
So the sprite still exists.

Projectiles in general are interesting, in that they have no internal logic.
They just change position, and collide with things (mostly walls).

So the physics wrapper provides us with a list of all the currently active *bodies*, which is not exactly equal to the currently active *sprites*.
It tells us their current coordinates, and also lists any active collision pairs.
(So it's not just a list of bodies; it has some internal structure.)

> Aside: The physics wrapper will probably have to query newly created bodies
  for their internal IDs to pass on to their models.

So actually what it does is return a list of *updates*, which include the positions of all bodies, and also collision events.

That's the input to the logic updater, as well as part of the input to the renderer.

> The other part of the renderer input is a list of sprite changes from the 
  logic engine, but that should be infrequent.

The logic updater can then run through this list and update registers accordingly. 

* Coordinate updates only matter for *agents* (player or otherwise).
  We then have to map the body ID to a model ID, and set the relevant registers.
* Collisions matter to everybody. 

Note that if a projectile collides with a wall, it is removed from the game.
Or the physics part, anyway. Its sprite might still hang around, in the form of a brief impact animation, but the rest of it is gone.
It is possible for the physics wrapper to recognize this situation and take action on its own, but that seems like it would break encapsulation.

We respond to collisions involving agents by putting notices on the agents' event queues. So we fill up the event queues at the top level, before going into the agent-level update loop.

Anyway, then the logic module runs and produces its own list of updates for the renderer. I think the only information that the logic module can produce that would be relevant for the renderer is a case where a new animation needs to be played. That is, a change of state that has a visible consequence.

Take the case where the model executes a `STRIKE 5` ("strike with energy 5") instruction. We might want the renderer to animate a sword swipe.
It might need even more information than that: maybe the currently equipped weapon is an axe, or a spear (which stabs instead of swiping). 
I guess there's a whole collection of weapon strike animations and someone has to pick which one to use.

The logic module doesn't know what a strike looks like, or if there even is an existing animation for a strike for this particular kind of agent.


### Removing Objects from the Game

In the case of a projectile hitting a wall, we know already from the collision record that the projectile must be removed from the physics world. 
So maybe the physics wrapper could handle that itself.
The collision is still important, because it may trigger a special animation, and at very least it requires removing a sprite from the graphics world.
So we still leave that in the list of updates.

Actually, any collision involving a projectile will cause that projectile to disappear. Again, possibly with a short exit animation.

But this represents a "responsibility leak".
Because cases where we have to remove an agent from the physics world only occur, or are only recognized, within the logic world.

And then finally there's the graphics world. 
How do we handle exit animations?
We have to know that, once we hit the last frame, we have to remove the sprite from the graphics world entirely.
I think in all cases once we start that exit animation, the corresponding body and brain have already been removed from their worlds.

What about explosions? 
Really in this case we are *replacing* one body with another, or a cloud of others. Importantly, it's still a physical body, and we still want to track collisions with it.
But it has a totally different density and so on.
Really, it has no place in the physics world at all, except as something that may be collided with. But the collision shouldn't cause any big changes in the trajectory of the colliding object.
So in this case we remove the original object from *all worlds*, and add a new object that may have a body, certainly has a sprite, and probably doesn't have anything much in the way of logic.

Note that if we have a wrapper around the physics world, 
then it should be possible to add our own special cases to it. 
So we might handle the physics of explosions with a totally different engine, with its own update function.

## Vision, Revisited

So we still have this communication problem. Namely that every update to the `aim` register or its cousins requires us to cast a ray in the physics world.

That's not *strictly* true. If the logic world knows X and Y positions of things, then we have all the data we need to resolve vision queries.
But it feels like another *responsibility leak* to me.

There's also the symmetrical case, where an agent moves into an observer's line of sight. That should lead to a spotting event, just like moving the vision sensor.
And the computations necessary to trigger the event are just the same.

I suppose it's fair to say that *vision is a brain problem, not a physics problem*.
So handling it in the logic world isn't so crazy as all that.
And so raycasting isn't a physics responsibility at all.

How to trigger events when an object swims into view is still an interesting problem, however.

* We could start every update cycle with a vision probe along the current 
  look angle. But if the object we are seeing hasn't moved, then it seems wrong to generate a new event in this case.
* Maybe this is something that represents too much realism, and we should 
  just ignore it?
* If it really matters, then we could pass in an "objects that have moved" 
  list from the physics update, and filter by that.

I think I vote for the second alternative: do vision probes only on either setting or reading a vision register.
Note however that JSRoboWar does do radar and range interrupt checks as part of its chronon-initial interrupt checks.

## Initialization and Object Creation

Player agents are created on game startup.
It is conceivable that, once we have enemy agents, they may spawn during the game.
Projectiles are spawned all the time.
I don't know about blows: they might not be independent objects? Have to think about those some more.

On startup, we have a list of player agents to create, from some sort of configuration data.
Creating an agent means creating it in all three worlds:

* Add it to the physics world (and get back its body ID)
* Add it to the logic world (and get back its model ID)
* Add an initial sprite to the graphics world (and get back an ID)

In the case of the graphics world, you get back the ID of something abstract that handles all the sprites for that particular object.
Like a *SpriteManager* or something like that.
*MetaSprite* maybe. *SpriteMaster*.

Then you have to make sure everybody knows all the other IDs.

For example, the renderer gets a list of position updates that is generated by the physics engine. 
Each record says that "[Body ID] is now at (X,Y)".
It is up to the renderer to figure out which sprite master corresponds to that body ID.
At some future point we might even want to add decorations to emphsize collisions, so the renderer would want the collision list as well.

### Removal, Again

So if a projectile hits a wall, and we want to make it look cool,
how does the renderer get the position of the projectile?
In theory, it has already been removed from the game.
The sooner the better, so we don't waste time having it show up on radar, for example.

I don't think the collision report says exactly what point is in collision.
Anyway, it might look odd, since the collision has been resolved by the time anyone gets a look at it. So the colliding bodies have "moved on".

> Though really projectiles don't hardly bounce.

I hope that's not really true, though, since we are counting on the projectile position to tell us point of impact. I guess there's a parameter you can set that will make something totally not bounce. Restitution, I think.

Maybe the physics wrapper can maintain its own list of bodies?
That is, a map from IDs to bodies, or `null`, in case the body has been removed?
We might need such a list anyway, since the physics engine might not want to expose an index like that. 
There may be no other way to look up a body by its ID.

Maybe we can do something like the following:

1. Physics detects the collision of a projectile with something.
   It adds a collision record to the log.
2. Logic determines that the projectile should now be removed from the game,
   in the collision handler for projectiles. It also adds a "set sprite to exit animation" to the graphics updates.
3. Graphics, as usual, gets the current X,Y and orientation of each object, 
   including the projectile, and switches to the end-of-life animation.
4. Physics (next time around the loop) starts its update by looping through
   the *to-remove* list, and removing objects from the world, before calling
   update on the actual physics engine.

So the point is that we do not actually remove anything until we have had a chance to set its consequences in motion. So we can store anything we will need later.

So, when do we remove the model?
Note that projectiles do have models, even though `update` is trivial for them.
We still have to know the projectile type and its payload of energy.

Also, it is possible that some weapons might have long-term effects. 
Bombs might do damage over the course of several turns, even after the physical bomb is no more.
I guess we go from being an object of type *bomb* to being an object of type *blast*.

I guess we could do the same *to-remove list* trick with the logic world too.
Though really it shouldn't be needed. It's exactly in the logic world that we discover that something should be removed, so we should be able to do it locally.


### ID Maps and Update Logs

Suppose the physics wrapper can actually manage to get a list of current positions, to feed the logic module.
That list is going to be in terms of Body IDs.
Theoretically, somebody somewhere knows that Body 1 corresponds to Model 3, since they were created together.
But that means an extra `map` step on the updates list.

Another problem with the log approach is that we may be recording things that no one needs to know, thereby doing extra work. 
And extra work is one thing we really want to avoid.

One very important thing about the update logs: 

> All information that anyone needs to know outside the emitting world is 
  contained in the log record.

So there should be no cross-world querying.

Is this really feasible? Consider raycasting for vision.
Well, the point is that X,Y coordinates are part of the model.
That is, we keep local copies, so that we can do exactly that sort of thing.

There's also an efficiency issue.
If the physics update creates a list of all the positions of everything in the game, every frame, then we are really close to making a full copy of the game world every time through, instead of just having a single copy that everyone can get at, but mostly only with read-only access.

Unfortunately, there's a kind of paradoxical situation here.
We can't have a single *model of record*, because the physics engine needs to do its own thing with its own data structures.
Even if we could make our own model data piggy-back off of the physics body objects, we would run into trouble when we wanted to remove an object from the simulation world.

> Actually, that might not be a problem. If someone else still has a 
  handle on the relevant body object, it will not be destroyed. Just removed from the scope of the physics update step.

There are still separation of concerns problems to solve, though. 
Even assuming a single object could wrap both the body and the model, 
and maybe even the sprite master, 
that would allow and encourage calling arbitrary methods and accessing arbitrary properties of things.

I think we should try the update log approach, and see if it's feasible under game constraints.

Now, how and when do we *clear* logs?
For example, the game logic can't *consume* the position update records, since the renderer presumably also needs them.
And we can't clear all logs at the end of every game loop,
because the logic engine will have posted some change-of-velocity updates that must be read in by the physics engine.
Also, the list of bodies to remove from the physics world.

There are certain properties of game objects that *everybody needs to know*.
Mainly these are:

* (x,y) position
* drive vector/angle (or "static")
* object type (agent, projectile, blast?, wall?, etc.)

Unfortunately, position cannot be updated by anything but the foreign physics library. Drive vector is the responsibility of the logic engine.
Probably really only the logic engine cares about type, 
but there may be some way to filter physics queries and the like according to user defined object classes, maybe?

So really everything we need to know is contained in the model, 
with the single caveat that the model must just echo the physics world's position data.

That means that, in order to render a game object, we need to have a reference to its model.

It's looking more and more like what we really have is a standard tree of game objects. The physics body component has two owners (the engine and the game object), but for our purposes the object is the owner of record, and the physics world just borrows its reference.

## Game Objects and Encapsulation

So for now a game object has the following parts:

* A physics body
* A model
* A sprite master

And the main Game Object includes:

* A physics engine
* An inferencer
* A renderer
* A hierarchical collection of Game Objects

The physics engine only cares about the object's body,
and the renderer only cares about its sprite master (and its position!).

The inference engine must tell the physics engine where our engines want to drive us, and it must tell the renderer when to change animations.

To a considerable degree, the "inference engine" isn't really a separate engine. 
It is just the implementation of `update` for sentient game objects (aka *agents*).
So game objects aren't just data. They come with behaviors as well.

So back to our classic problems.

### Collisions

Periodically, during the physics update, we will be notified of collisions.
We will be told which *bodies* were involved, but we will not automatically know which *game objects* were.
So we need an efficient way to get from a body object to a master game object.
One obvious choice would be a reference, but that creates a reference cycle, and doesn't that cause problems with garbage collection?

> No, apparently it does not. Modern garbage collectors do not use reference
  counting, and they can recognize these kinds of cycles. If nothing else refers to the objects in the cycle, they will be garbage collected.

So we actually can add a `body.gameObj` property. We don't have to use indexes or IDs.

### Death and Destruction

The update discovers that our agent now has zero health, or whatever other condition would warrant it's removal from the physical plane (e.g., it hit zero health *n* turns ago).
Removal from lists is really problematical if we use array indices as references anywhere.
So we have to be really careful about that.
But if we use references for references, maybe not so bad.

So, `PlayerAgent.update`:
```
if (this.model.health <= 0) {
	game.physics.removeBody(this.body);
	this.model.isActive = false;
	this.graphics.setAnimation('dying');
}
```

### Drawing Layers

Not only does the renderer have to know which frame of which animation to draw, where, and rotated in which direction, 
it also has to know which drawing layer to render it on.
Currently this is done by looping through all the objects once for every layer, and skipping over objects that don't draw to the current layer.

It would be nice if we could just move the sprite master from one layer to another. That is, if layers were collections of sprites.
But with a not-dead agent, we have to draw to two layers.

The correct way to do it is probably to have multiple canvases.
I think there is a `z` property that specifies rendering order.

Otherwise the only thing I can think of is separating the generation of objects to draw from the actual drawing.
So first `render` creates a list of drawing instructions, really three lists that get concatenated together, and then it maps over the list and executes them.

That looks suspiciously like three loops, but note that the lists will be quite a lot shorter than the whole list of game objects.
And they will be generated in a single pass over the object list.
So "drawing to layer 1" means "add drawing instructions to sub-list 1".

What is a "drawing instruction" in this case?
For a player agent, there's a few separate things that might get drawn:

* The player body
* The aim indicator
* The health bar
* The label

If the agent is dead, then the health bar doesn't get drawn, and everything else gets colored gray. 

In JavaScript, it is possible to call `apply` on a function object, with an array of arguments.
So a drawing instruction could literally be something like:
```
{
	op: ns.drawHealthBar,
	args: [x, y, percent]
}
```
And rendering is then just:
```
instr.op.apply(instr.args);
```
Emitting the instruction could be something like:
```
preRender: (layers) => {
	if (this.model.isActive) {
		let pos = this.getPosition();
		layers[Layer.ACTIVE].push({
			op: this.renderBody,
			args: [pos.x, pos.y],
		});
		layers[Layer.LABELS].push({
			op: this.renderHealthBar,
			args: [pos.x, pos.y, this.getHealthPercent()],
		});
		layers[Layer.LABELS].push({
			op: this.renderName,
			args: [pos.x, pos.y, this.name],
		});
	}
}
```
As an optimization, note that any drawing to the bottom layer can be done immediately.
The key goal is to delay the execution of drawing instructions directed at the higher layers.
So then the top level looks like this:
```
layers = [[], [], []];
objects.forEach((obj) => obj.render(layers));
for (var i = 0; i < 3; ++i) {
	layers[i].forEach((i) => i.op.apply(i.args));
}
```
Probably we want a separate `createLayers` function to make sure we get something that is compatible with the layers we have defined.


# 2017-09-06

## Events

If we communicate using events, then every event type has to have a set of listeners, and every event of that type has to be published (somehow) to all its listeners, and then removed.

It would be nice to have game data be global readable,
but if so we have to control writing somehow.
Maybe writing is only possible for game data event handlers,
so whoever's in charge of the game data component gets to control when things get written. 


## The App

So what happens when you first navigate to a MAGIC site? 
What do you see? What can you do?

The main thing you can do is to issue the command "New Game".
But there may be some other things available:

* Replay from log
* Upload player agents
* Select rules/engines/game type
* Upload/Select a map
* Configure (and run) a tournament

Maybe some other stuff.

So the main thing we have to do on startup is to enable the UI to receive these various commands. 
That may not be very interesting. 
There might just be a big shiny button in the middle of the screen labeled "New Game!". 

In that case, everything starts with that button's `onclick` handler.
Which is essentially our current `main` function.
Only for now it's rigged to start automatically on window load.

But maybe we really should call it `newGame`, not `main`.

By the time you hit the New Game button, 
it should be clear what sort of map and arena/viewport should be presented to the user.
I guess there will also be some *transport controls*. Though I doubt "Rewind" will be one of them! Just "Play" and "Pause", really, with pause being equivalent to stop.

In the initial case, the map is just four walls around a 800x600 arena.
The viewport takes in the entire scene, so we can ignore it for now.
Also, the roster has been pre-selected.

So the main thing we want to do is to construct a Game object, 
and tell it to `start()`.

## Game Objects

Let's start with *walls*.
Each wall is a game object. 
It has a body that participates in collisions.
It has a sprite master that knows how to render it.
It doesn't have much logic, though, and probably doesn't participate much in the logic update cycle.

> That could change if walls can take damage and be destroyed.

The assumption is that, in the simplest case, every call to `update` and `render` branches recursively to all the game objects.
And each game object has `update` and `render` methods.

Note that the physics engine has its own internal loop, so the master update method just calls `physics.update()` and that's the end of it.

Note that JSRoboWar splits game objects in two, so we have for example Robot and RobotView, or Arena and ArenaView.

But here it looks like we want something like
```
class GameObject {
	constructor () {
		this.body = {}
		this.model = {}
		this.sprite = {}
	}
	update() {}
	render() {}
};
```

Then I guess they get added to collections attached directly to the top level Game object.

This does leave us with some inefficiencies.
For `update` and `render` we end up looping over the entire collection of game objects. It is conceivable that there could be things to draw that have no logic or physics, and vice versa.

This might get critical later on when maps can be bigger than our viewport.
Then maybe we really don't want to render stuff that is offscreen.

If there is a collision with a wall, will the inferencer need to know anything about the wall? 

How do walls even draw themselves? How do they get their X and Y coordinates?
Do they have to call something like:
```
this.game.logic.find('NORTH').getPosition();
```
That would be awful. It requires knowing so many APIs that you have to pass through.

Another problem with position is that it's sometimes in different coordinate systems. Like Matter uses body centers for position, but for drawing a rectangle we'd most like to know its top left corner (min x, min y).


### Game Objects and Collisions

If there is a collision, then the inferencer will get a collision event record.
If it's just straight out of Matter.js, then it just mentions the body objects that collided.

The physics wrapper should translate raw Matter collisions into collision reports written in terms of game objects.


### Drawing Objects

Just take as an example the four walls of the arena.
The "map", in other words.

A map is a composite object consisting of various wall segments.
In our case it's super simple: there's four of them.
They are rectangles, with a particular position.
Their size is based on the global arena size, and a global constant wall thickness value.
Their color ("texture") is known only to the renderer.

When we create each wall, we have to create a physics body and a drawable
with code to draw a rectangle in the `GROUND` layer.

```
addNorthWall(map) {
	let width = map.outerWidth;
	let thickness = this.game.const.WALL_THICKNESS;
	let northBody = Physics.wallSegment(
		width/2, thickness/2, width, thickness,
		{ isStatic: true, label: 'NORTH' }
	);
	let northView = ns.Graphics.rectangle(
		0, 0, width, thickness);
	map.addBody(northBody);
	map.addView(northView);
}
```
Maybe:
```
addWall(map, x, y, w, h, name) {
	let body = Physics.wallSegment(
		x + w/2, y + h/2, w, h,
		{ isStatic: true, label: name });
	let view = Graphics.rectangle(x, y, w, h, name);
	map.addBody(body);
	map.addView(view);
} 
```
So it looks like `map` might have a body, and also a list of sub-bodies, and a view, and also a list of sub-views.
This does not make me happy.
But if you want to remove a map from the game entirely (or some other composite), then you will need to be able to find all those other parts.

Really, a map should have a body which is a composite, and a view which is also a composite.
I don't know how this would work for physics, though.
Maybe a map isn't really a thing.
It's just instructions for adding some things, that otherwise have an independent existence.


# 2017-09-09

## Rendering Sprites

In the current state of development, "sprites" are not sequences of bitmaps from a spritesheet. We actually follow a sequence of drawing instructions.

So the "sprite" is actually a function.
And when the actor dies, that function is replaced by a different one.

So instead of having a keyed list of static assets,
our crude sprites have (I guess) a keyed list of functions.

There's two of them, corresponding to states of the controller,
so we can call them `DEAD` and `NOT_DEAD`, I suppose.
They each need to know, as arguments:

* Sprite position
* Sprite aim
* Base color

Well, only `NOT_DEAD` actually needs to know the color.

You know, I'm having my doubts here.
Having all of that stuff together as the properties of Actor States
was really handy.

If we really want to control what gets drawn, to save looping over all objects, then we can have a separate list of *visibles*, which would just be borrowed references to the actual objects controlled by the main game logic.

Likewise a separate list of *updatables*, to control the length of the list we map `update` over. 

But those are optimizations that would be premature at this point.

Also, I think distinguishing Game from GameLogic isn't getting us anywhere.

Having a Graphics object around is still useful, since it provides a custom drawing context (LayeredCanvas), and potentially also a wrapper around whatever graphics library we might eventually use.

But what about when we do have a whole spritesheet to manage?
Including which particular image we are currently using, and other animation data?
It will be very convenient to have a separate object that can deal with that.
The only real change here is that that object will be a member of a master Game Object.

So `render` is called on the game object, and it delegates to its sprite master. But in the meantime we can pick up some data from the model to pass down to the sprite master, so it doesn't have to have an internal reference to the model.

The big thing is that, without a separate graphics library,
we have to add the physics body to the physics engine, 
but we don't have to do anything special with the sprite master.

### Handling State Changes

The game knows only game objects.
So when it calls `update` or `render`, it calls them on the game object.
Game objects know about states, and sprites.

So, when a call to `Actor.update` causes a state transition,
from `NOT_DEAD` to `DEAD`, or from `DEAD` to `ELIMINATED`,
this can in turn change the current sprite.

In general, `update` is always able to change the current sprite,
by requesting a new animation.
So the sprite has to always be controlling a collection of animations that it can switch between.

> Generically, an "animation" is just an object that actually knows how 
  to render its controlling object. No delegating, no buck-passing. 
  Just actual graphics world drawing instructions.

It follows that all drawing instructions should be located in Sprite classes of some kind.

In a future world, we may be able to construct a sophisticated "just in time" resource manager, so that the sprite can load resources only when they are needed, and can cache old resources that are likely to be reused.
But for now, a sprite on construction will have a full list of all the separate animations that it might ever need.

So, on state change, a game object has to be able to tell its sprite master to change animations. I guess by passing it some kind of shared key.

That means in turn that the architecture of our previous State classes changes.
The result still seems kind of sloppy to me.

```
PlayerAgent.prototype.update = function () {
	let result = this.state.update();
	while (result === Q_DONE) {
		this.state = this.transition(this.state.id);
		result = this.state.update();
	}
}

PlayerAgent.prototype.transition = function (q1) {
	switch (q1) {
		case Q_NOT_DEAD:
			this.sprite.select(DEAD); // <---
			// And request removal from the physics world...
			return new StateDead(this);
		case Q_DEAD:
			this.sprite.select(ELIMINATED);  // <---
			return new StateEliminated(this);
		default:
			throw `No transition out of state #${this.id}`;
	}
}
```

Now suppose that we have a special sprite for "walking north east".
Is it the responsibility of the model to know that (a) we are now headed close enough to NE, and (b) there is a sprite for that?
It seems like this should be part of the *render logic*.
And the render logic should be encapsulated in the sprite.

So maybe it is the sprite's responsibility to know that it has a "dead" animation, as well as a "not-dead" animation.
And so in `render` we query the model's state ID, and branch based on that.
(It could be any aspect of model data, like "health is 0", or "heading is NE-ish".)

Now the fact that there is special "I'm dead" *logic* and "I'm dead" *graphics* is completely accidental. The fact that they both change at the same time is not special in any way.
And that kind of makes sense, given that there is no special logic for walking northeast.
It's up to the sprite designer to know what resources they have, and how to deploy them.

Then the question becomes how intimate can the sprite get with the model?
How much access to model properties does it get?
I guess the short answer is "Methods only!", no direct member access.
But that's pretty weak: we can always make a new method.
But at least that gives a little separation, allowing field names and structures to change in the model, without changing the API.

So that means that we don't need separate state classes at all.
Rather, we need an `update` method that can branch to `notDeadUpdate`, `deadUpdate`, and `eliminatedUpdate`.

I'd still like to at least try to implement *Behaviors*, which would basically be the update component of the previous agent states.
I think that would be a very useful abstraction, 
and it doesn't seem like it leaks out of the game logic, 
though there might be sensible graphical changes (like if we load a new "run for your life" behavior...)

So we still have our trio, of not-dead behavior, dead behavior, and eliminated behavior.
Basically they are just functions, but we can attach state to them, since they are also objects.

So *dying* would now go like this.
At the beginning of the actor's `update`, check health.
If it's zero, and we are currently exhibiting not-dead behavior, 
then load dead-behavior.
At this point, we should remove our body from the physics world, or schedule it for removal.

Maybe have `Actor.update` return a list of game-level tasks?
The only ones we have at the moment are "Remove X from physical world" tasks.
But there could be others.


## Vision

So I am implementing my own line-circle intersect routine, based on what I found on the web.
If there is an actor along the cast ray, then an interrupt is triggered.

It occurs to me that switching behaviors ought to include the possibility of changing what is registered as an event handler.
Also it may be that event handling should be turned off sometimes for particular events.


# 2017-09-08

## Programming Language

An interesting note from looking at the Lua VM (which is a lot like three address code), regarding *conditional jumps*. In order to save space in the actual opcode fields, a conditional like:
```
if (x < 0) B1 else B2
```
is implemented in a sequence of two instructions, roughly:
```
IFLT R(x) K(0)
JUMP B2
B1: ...
B2: ...
```
So if the `IFLT` instruction succeeds, it bumps the PC ahead *by two*, and if it fails then it falls through to the `JUMP` instruction.

Coding programs at a machine-like level is one way to allow varying CPU speeds in a fair way. But it is probably not the only way.

Abstractly, an actor has a given number of "thought points" to spend on each turn. The language could be high level, as long as it is clear how many "TP" each expression costs.

> Also, you need to be able to pause execution after a turn's TP 
  have been exhausted, which may not be equally easy for all languages.

One possible approach to finding the right language might be to stick with JavaScript for now, but use a separate Program object. So all interrupt handlers, storage, and so on, would be held by the program.

Then have `Actor.update` call `this.program.update(TP)` or something like that.
We can ignore TP for now, but the program has to be structured in per-frame chunks.

Every chronon, the program gets an event queue to deal with.

Actually, for interrupts, we have to check the queue on every CPU step.
Though in fact only certain interrupts can be generated in the middle of a chronon. Pretty much namely only those triggered by the vision system.

And also the energy system, now that I think about it. If you have an interrupt set to trigger if energy drops below threshold, then firing or shielding or even maneuvering could trigger this.

Nothing special has to be done here, however, since we still always process all interrupts on the queue before every step (when interrupts are enabled).
It's just that not many interrupts will be added to the queue chronon internally.
It still might matter as an optimization if we have the queue broken up into sub-queues of interrupts of particular types. It could take less time to loop through the possible sub-queues.

Regarding programming languages, I seem to recall that a design principle for BASIC was that it should be close to assembler language, so that no instruction should take that long and it would be easy to interpret.

### 3AC Based Language

Operations are basically like what's in RoboTalk, but the syntax is 3AC rather than stack based.

One trick is going to be interpreting labels, in particular in the case of *forward jumps*.
The usual approach would be *back-patching*, where we insert placeholders for labels and replace them later.
Another possibility is less efficient but might be okay, which would be a *resolution table*. So whenever we encounter a label for the first time, whether as an l-value or an r-value, we add an entry to a map taking it to the next avaiable location index. And we have an array over location indices that we fill in as we find `LABEL` statements.
We still can't actually interpret r-value occurrences until we have encountered the l-value occurrence, however.

A lot depends on whether we are compiling, or interpreting.
If the latter, then we absolutely need a way to find the target of that `JUMP` we just encountered. I guess in that case you just scan the token list forwards from the `JUMP` site until you find a matching `LABEL` statement, and you continue executing from there.
The problem then becomes *back jumps* to labels that you skimmed over while looking for this label. You have to keep some kind of track of where they are.

One trick that might work is to treat addresses as belonging to *tokens*.
Then, whenever scanning for a particular label, you can cache the token addresses of any labels you encounter during that search.
And the representation of the program becomes an array of tokens, not an array of instructions.
Since tokenization should be pretty fast, that might work.

One problem with 3AC is that we don't have any notion of blocks or local scope. So our register count just keeps going up. 
Or, to put it another way, *all variables are global*.
So probably best to just bite the bullet, and instead of pretending we are using registers, allow the programmer to freely choose names for temporaries.
Then if we ever compile a program, we can try to be clever about register allocation.

Okay, so what about `CALL` with arguments?

* Push the arguments into a list, push the list onto the call stack.
* Jump to the indicated address, adding the return address (PC + 1) 
  to the data on the call stack.
* On any `RETURN`, set the PC to whatever's on the stack, and pop the 
  activation frame (probably a single JS object).

Technically, this gives us the opportunity to define temporary local variables, by storing them in the activation frame. Allowing us to reuse the same symbols later.

Consider the case of recursion. 
The same local variable may be encountered on multiple separate calls.

Now the problem becomes accessing *global* variables in the body of a function. 
Especially brand new ones. How do we know not to allocate them in the stack frame?
They have to be specially marked, somehow.
Kind of like Python?


# 2017-09-09

## Language (Cont'd)

I guess if you are going to use global variables, 
then you need to declare them. With declarations not counting towards CPU cycle budget? (Like labels.)

On the other hand, note that we don't really have any such thing as functions. `CALL` just directs us to an arbitrary (maybe even computed) location and stores a return value.
The main point being that there's other ways you could get to that location, like a straight `JUMP`, or falling through.

But that's no good either, because then the arguments aren't in place. It only works for functions taking no arguments.

So it looks like allowing `CALL` with arguments is a real watershed decision.

The alternative is to use regular registers for call arguments.
This means there can't be any recursion or anything like that.

Otherwise, the program has to be structured as a collection of functions.
And it is up to the interpreter to assure that there is no way to fall out the bottom of the function code. It's okay to generate an unreachable `RETURN` instruction. Code size is, so far, unlimited.

Otherwise, how would we implement something like:
```
if (range > 0) then
	fireSub()
else
	rotateSub()
end
```
It would have to look like:
```
GT [range] 0 A
IFNZ A B1
CALL RotateSub
JUMP B2
LABEL B1
CALL FireSub
LABEL B2
```
Which is also what it would have to look like if one of the if-blocks contained a sequence of more than one statement.

### A Higher Level Language

Suppose we take a step towards Lua? You'd actually be able to write something more like:
```
function main() 
	loop
		if (range > 0) then
			fireSub()
		else
			rotateSub()
		end
	end
end

function fireSub()
	fire = 20
end

function rotateSub()
	aim = aim + 5
end
```

Probably the only looping construct would be `loop...break...end`.

And compilation would be to a collection of "function prototypes", which would get referenced by actual "function activations" each time they were called.

And so now you would have a stack frame, and you could poke local variables into it if you wanted.

The thing is, if you don't implement support for activation record handling in the virtual machine itself, it's not something you can easily patch in later. 
Almost everything else is just a matter of adding fancy new instructions, but this really has to be built in.

### Compilation Target

So a compiled program is no longer just a sequence of instructions.
It is at its core a *table of function bodies*.

Then for each function we need instructions on how to call it, that is,
how to construct its activation record. I guess mainly that's a matter of figuring out how many slots to reserve for arguments.
Then, if the frame includes local variables, we need to plan for those.

Functional style `let...in...end` constructions are really good for handling local variables.
It means that every function activation becomes a *stack of variable bindings*, basically, potentially one for each nested block.
With the function arguments as the outermost block, essentially.
Almost like a function call is sugar for
```
LET arg1=v1, arg2=v2, ... IN 
	JUMP functionBody 
RETURN
```
Anyhow, this is all about higher level languages, and for now we need to focus on the virtual machine language.

### Parsing

So suppose we have an instruction `GT range 0 A`. 
What parsing problems does it pose?

1. We know the expected types for `GT`: Number, Number, Local.
2. We can see that `range` is not a number.

One way to resolve `range` would be by looking it up. Did we use it as the name of a local? No, keep looking. Is it a user-defined global? No, it is not. Is it a register? Yes, it is.

Another way is to actually tag expressions in the code, so the tokenizer can already tell what type it is.
So maybe all locals are of the form `T_1`, `T_2`, etc.
Or maybe all registers are of the form `.range`, `.aim`, etc.

I guess all particular values belong to some known collection. Maybe we just name the collections.

* Locals: `L.1`, `L.2`, ...
* Registers: `R.range`, `R.aim`, ...
* Arguments: `A.1`, ...
* Globals: `G.1`, `G.2`, `G.myName`, ...

If `L`, `R`, and `G` really are just JavaScript objects, 
then we are free to call our own variables anything we want.
I suppose we could even add "hidden registers", they just wouldn't mean anything.

So now what about nested scopes? `Let...in` blocks?
Now a local like `L.var` can't just be a field in a globally known object `L`.
It might be found in the environment at the top of the stack, or we might have to search deeper.

Instead, ultimately it has to be a function call, a call to a function defined by the VM, meaning "Search the scope stack for something called `myVar`."

So:

* Register names are references to global variables of the VM
* Globals are references to a user defined globals section
* Locals are references to the first environment in which they are 
  defined.
* Arguments are references to the first environment that is of type 
  "activation frame", rather than of type "block"

So maybe `Sys.range`, `User.myGlob`, `$arg`, and everything else is a local.
We could just skip the special `$1` notation for arguments, if the function definition provided conventional names for them.

If you're actually writing in virtual assembler, 
is there any such thing as nested blocks?

* There's no way to re-declare a local variable, so that the instance 
  in this scope shadows one that was declared in an outer scope. You really 
  have to work that stuff out for yourself and make sure all names in a 
  function body are independent.
* There's (currently) no machine instruction for introducing a block. 
  Hmm... Can we even implement block structure without machine program 
  level support? I guess it's just the compiler doing the same "naming apart" step that programmers would otherwise do on their own.

So maybe there needs to be `LET` and `END` instructions?

* Like `ARGS`, `LET` could take any number of arguments.
* `END` just means "pop the top of the environment stack".

Then every `if...then` would introduce a `LET...END` for each branch.
But if you're programming in virtual assembler, that isn't a problem.
And even at higher levels, it's just a problem for the compiler to figure out when "block elimination" is possible.
Note that then it is not the case that every source level `end` keyword is translated to an `END` VM instruction. 
Maybe it would be good if they didn't have the same name...
`LOCAL vars ... POP`, or something like that.

Maybe we can make those instructions free, to encourage (or at least not discourage) structured programming.

Or maybe we just insist that if you are going to write low level code you have to be careful with your variable names.

You have to be careful about things like *variables local to loop bodies*.
These don't get removed and then reintroduced every time through the loop.
So the translation of a loop construct with a loop-local variable would have to include variable initialization every time. 
Or else just insist that users always initialize variables properly or suffer the consequences.

`ARGS` values *must be either constants or locals*.
Like any other 3AC, there's no expressions in the language.
Even though technically this is more like nAC.

Maybe make `CALL...ARGS` into a construction.
Whenever you encounter a `CALL`, scan ahead until you find a matching `ARGS`, or something else.
Hmm... That sounds like a parsing nightmare.
You can find a `CALL` with no matching `ARGS`,
but you will never find an `ARGS` without a matching `CALL`.

If we interpret the construction immediately,
then `ARGS` pushes an environment frame,
then `CALL` pushes an activation frame, with a return address and a storage location for the return value.
Without special pleading, those two objects have nothing to do with each other.
But since `ARGS` can't appear on its own, we have a basis for that plea.

I suppose it could be as simple as setting a flag. 
If raised, the `CALL` interpreter knows to augment an existing stack frame.
Otherwise it introduces one of its own.

Basically, when the interpreter scans an `ARGS` token, then it is already in "prepare for function call" mode. 
In that mode, the `CALL` is really just punctuation to mark the end of the arguments list.

Stack frame, so far:
```
let frame = {
	argtoks: [],    // referenced by position, $1, $2, etc.
	locals: {},		// referenced by name
	return: -1,		// Need a better default. End of program?
	retval: nil,
	type: 'function-call',
};
```
The default return address should probably be a location with a single `ABORT` instruction.

By convention, `$0` will probably be the call site. Not really useful, except maybe for debugging.

### A Dry Run

Given the program:
```
LABEL Main
	GT Sys.range 0 A
	IFNZ A CallFireSub CallRotateSub
LABEL CallRotateSub
	ARGS 5
	CALL RotateSub 	# no return value to store
	JUMP IfEnd
LABEL CallFireSub
	ARGS 20
	CALL FireSub 	# no return value to store
LABEL IfEnd
	JUMP Main

LABEL FireSub
	STORE args.1 Sys.fire
	RETURN

LABEL RotateSub
	ADD Sys.aim args.1 Sys.aim
	RETURN
```

General invariants we might want to maintain:

* PC is always the program address of the next opcode.

1. Scan token `LABEL`
	1. Save the current PC value
	1. Scan ahead one token: `Main`
	2. In the label map, add entry `Main: StoredPC`
2. Scan token `GT`
	1. Expect 3 arguments, so scan ahead 3 tokens
	2. Branch on type of arg 1: It's a register name, so fetch the contents
	3. Branch on type of arg 2: It's a number, so we're good
	4. Compare the two, with result either 1 (true) or 0 (false)
	5. Store the result in arg 3, which must name a storage location
	   (i.e., a proper l-value)
3. Scan token `IFNZ`
	1. Scan 2 tokens. Arg 1 should be a storage location or a constant. 
	   If its contents are anything other than zero, then set the PC to the value of arg 2, which must be a location.
	2. If arg 1 is zero, then scan 1 token. If it is a location, set the PC 
	   to point to it. Otherwise it has to be the next opcode.
4. Scan token `ARGS` (assuming `A > 0`)
	1. Construct an empty stack frame
	1. Scan token. If it is not `CALL`, then it must be a storage location or a
	   constant. Push it onto `frame.args`. Repeat.
	3. On scanning `CALL`, drop any previously stored tokens, and go ahead 
	   and scan the next token after it.
	   It must be an address. Store it as `jumpTo`.
	4. Scan the token after that. If it's another address, then it is our 
	   return address, `frame.return`. If it's storage, then it's our return value, `frame.retval`. If it's an opcode, execute step 6.
	5. If the last token was an address, scan ahead 1. If it's storage, 
	   set `frame.retval`. 
	6. If `frame.return` is not set, set it to PC (of the last scanned token).
	7. Set PC to `jumpTo`.

Okay, here's a problem. The argument of `CALL` is not an address, it is a label.
Same problem with `IFNZ`. 
We can't jump forward until we have the ability to decode that label.

If we really want to mix tokenizing with execution, then we can deal with addresses by scanning ahead until we find them, tokenizing the program in the meantime.
But maybe to start with we just do a complete tokenization pass, 
and then interpret the token sequence.

Basically:

* A program is a *sequence of instructions*
* Every instruction *begins with an opcode*

So the scanner is really all about finding opcodes, and breaking the token stream into "lines" on opcode tokens.

* I think everything that is not an opcode is either a *symbol* or a *number*
* A symbol is either a *storage location* or a *program location*.
* A storage location symbol can be complex, consisting of
  a *sequence of path components*, separated by a '.' character.

So the name of a storage location is, in general, a list of strings, not a string.
So the value of a token can be either a number, an opcode, or a list of strings.
Maybe for simplicity we just let it be a string, and split it later, when we actually have to decode it.
So we have:

* Type: opcode, value: string
* Type: number, value: JavaScript number (float)
* Type: symbol, value: string

### Decoding instructions.

We want a function that will take an instruction's *token sequence* and convert it into the actual values that we need to operate on. 
So `GT sys.range 0 A` wants to be a decoded sequence like
`['GT', (contents of range), 0, frame.locals.A]`.

There is a problem with `ARGS` instructions: we don't know from the opcode what types we need to derive. 
So I guess we have to load the frame with the raw list of tokens.
And then decode them as they are referenced in the body of the function.
Though we should make sure not to decode them more than once.

But think about it.
A token is either an opcode, a number, or an identifier.
The decoding of opcodes and numbers is trivial; it's just the identity function.
So the only thing tricky is decoding the identifiers.
And we don't need to know how they are used to know how to decode them.

An identifier can only be one of:

* A storage location
* A program address label

But we do need to know if an identifier is being used in an lvalue or rvalue context.
If it's an rvalue, then we have to fetch the contents of the storage location.
If it's an lvalue, it's a reference to the actual location that we need.

Unfortunately, an actual JS reference to a location like `this.bot.aim` is going to be treated as an rvalue in this context. We can't pass it as an argument to some instruction interpreter, and expect that it will arrive as something we can assign to. In the case of `bot.aim`, it'll arrive as a number.

We could pass the pair `(this.bot, 'aim')`, and then assign to
`this.bot['aim']`...
So storage locations are always *pairs*, consisting of a *container* and a *key*.

Possible containers are:

* `bot.sys`, called just `sys` in the token
* `globals`, called `user` in the token
* `frame.args`, called `arg` in the token
* `frame.locals`, not having any prefix at all

I think it's just `this.globals`, but I might have lost track of what `this` is in the relevant context...

But still, a register (for example) passed as an argument to a function could be used in that function as *both* an lvalue and an rvalue.
That is perfectly reasonable.
So it's still the case that function arguments have to be passed as *raw tokens*.

So, suppose you have a line like `store args.1 sys.fire`.

1. Decode `store`: Easy, it's just an opcode. So its value is whatever is in
   the token's `value` field.
2. Decode `args.1`: Get the object at `frame.args[1]`.
   This will actually be another token, so we have to call 
   `decode(frame.args[1])`, basically.
3. Decode `sys.fire`: This is in an lvalue position, so it is decoded as 
   an object. Something along the lines of
   `{container: this.bot, key: 'fire'}`.

I guess for consistency `decode` should be a recursive procedure in general.
Whenever you load from a storage location, there is a chance that the value you read is another storage location or other expression that needs decoding.
So far the arguments list is the only place that can happen, 
but it is theoretically possible that we could want to store the name of a register in a global variable, or the name of an opcode even.

### STORE and LOAD

In RoboTalk there is an instruction `RECALL`, which is mostly implicit. 
If you use a register in an rvalue position, it gets a `RECALL` prepended.
It's actually more complicated than that. 
If you write `AIM'`, you are referring to the literal location,
but if you write `AIM` unquoted, you are requesting a recall.

So in a way we are cheating--stealing a cycle--by not having a `LOAD` instruction to match our `STORE`.
Would it simplify things to insist that all non-local variables be loaded into a local 3AC register before they can be used?
So they could appear as lvalues, but not as rvalues. (Except in a `LOAD`.)

On the other hand, we are also cheating by using 3AC, so that `STORE` is mostly implicit, and not often needed.


# 2017-09-10

## Programming Language

### CALL Instructions

The syntax for this is a little screwed up, because there are so many forms it can take:

1. `call (func)`
2. `call (func) (retval)`
3. `call (func) (continue)`
4. `call (func) (continue) (retval)`

So if there are two arguments, the type of the second argument is ambiguous.
Simple decoding won't help, because we don't even know if the argument is an rvalue or an lvalue.
The token type is of no help, since they are both identifier tokens.

Probably the best we can do is to look up the token value in the label map.
If it's there, interpret it as a label.

That causes problems if the programmer has unwittingly used the same symbol as both a label and a local variable. 

> I think that's already a problem, though. Instruction decoding is blind 
  to whether it should expect storage or label.

Would it make sense to be able to pass a *hint* to the decoder?
Yes, in most cases.
But in this case we still don't know what the hint should be.

Another solution is to add a new instruction for *call-with-continuation*.
Isn't that traditionally already called `CALLC` in the literature?
Or am I thinking of something else?

> It's "`call/cc`", for "call with current continuation", and it looks 
  pretty messed up from way out here... Related, but not the same thing.

So I'll try `CALLC` for now, risking the ire and confusion of anyone who reads it as `call/cc`. Maybe `CALLB` for "call-branch".

Mind you, the only real use for this is to allow a function definition to remain unchanged whether you call it or jump to it.
Otherwise, you could just make one of the arguments a label, 
and exit the function with `JUMP arg.3` or whatever.

## Interpreter API and Top Level Control

Currently, a *program* is an object with the following general schema:
```
{
	instructions: [instruction,...],
	labels: {label: address,...},
}
```
*Instructions* are objects like:
```
{
	location: address,
	tokens: [token,...]
}
```
And a *token* is just a pair
```
{
	type: ('OPCODE' | 'NUMBER' | 'IDENTIFIER'),
	value: (number | label | storage-location)
}
```

The *Scanner* creates a list of tokens,
which is then *parsed* into a list of lists, such that each sub-list starts with an *opcode token*.

An *agent* should be an object that includes at least its *source code text*.
The scanner/parser should turn that into a *program*,
and attach that program back to the agent.

When an agent is added to the game, its source code should be compiled to a program.
Then, when the game starts, the game engine runs through a sequence of *update cycles*. 
Each update cycle includes advancing the programs of any agents that are currently functional by one step.

### Hooking Up Hardware Registers

It is supposed to be the case that writing to the `fire` register causes a projectile to be launched.
So the interpretation of `store 20 sys.fire` can't actually be to just write the number 20 to some object member.

In general, it is probably wrong to treat hardware registers as memory locations, since we don't really have active processes polling those locations for changes and reacting.

I suppose one solution would be to use the *task system*.
Post an event, and let the game engine figure out who should handle it.

Projectiles don't actually get launched until chronon breaks, so we can accumulate a list of them and pass them on to a bulk handler.
But changes to `aim` and company have to be handled immediately, 
so the event system doesn't seem like such a great choice there.

An alternative would be to make storing to lvalues that are hardware registers into a big switch on the register name, with case logic that can call methods of the parent agent.
That would mean changing how lvalues are represented.

This probably goes for reading registers as well.
In general, we discourage direct access to agent properties, 
preferring to use getters and setters.

That might be a good argument for having separate `LOAD` and `STORE` instructions for hardware registers.
`RLOAD` and `RSTORE`, for example.
But maybe it's easy enough to hook in to `decodeRVal`.

## File Organization

The interpreter should be more or less built into the Actor library.
But currently Actor stuff is in `game.js`, and VML stuff is in `Interpreter.js`.

There could be other tools that used the same Actor (e.g., editing and testing tools). An actor with the same loadout as for robowar could conceivably be programmed to run a maze, for example.
It's certainly true that I am far away from being able to factor out a RuleSet or GameType or whatever, so this isn't a pressing issue,
but "Aim high in steering!"

Signs point to moving actor stuff to its own file (like the old `PlayerAgent.js`). So then we have to consider how to get information back and forth between `MAGIC.Agent` and `MAGIC.Game`, basically.

Like for example all those register setters and getters have to be public methods of Agent, pretty clearly.
But I think they are already.

More awkward is that Agents are created with a reference back to the game.
They are game objects, after all.
And they use that reference. E.g., for looking around, and really anything that involves interacting with physics or graphics (and eventually sound).

> Actually, it looks like `checkLookEvents` is in fact the only Agent
  method that refers to `this.game`.

One thing that is peculiar in JavaScript is the ability to extend an object with new properties.
So I suppose it would be possible, in `game.js`, to define a new `checkLookEvents` method of Actor and also add the `this.game` property that it needs.

It's really only through the *hardware* that the agent and the game interact.
So I suppose it might be possible to snip out a Hardware module, 
and refer to it in both the game and the interpreter.
But the hardware on its own can't define things like reading the `range` register without the game's help.

> Basically, the interpreter is the agent's brains,
  and the hardware registers make up its sensory system.

The external API is (I think) mainly reduced to *construction* and the `step` function.
Where construction includes source program compilation.

The main connection is fairly simple. On every call to `update` we have to do something like:
```
update = function () {
	if (this.functional()) {
		this.interp.step();
	}
}
```
Also we have to handle interrupts, (some of) which would be collected by the main game engine, and some by robot hardware self-checking.
None of that is under interpreter control, 
only the response.

## Edge-Triggered Interrupts

If you want to bounce off of walls and not keep banging into them, 
then we have to set up events so they only fire when they are new.
So you only get a Wall Event on the first chronon in which you are in contact with the wall.


# 2017-09-11

## Frames per Chronon

We might be in a position where we need to slow the game down.
But trying to have multiple frames per chronon introduces hard problems.
We either have to show the same frame several times, 
or we have to advance the physics simulation faster than the robots' ability to respond.

First off, the robot's drive vector has to be in distance per chronon, 
because chronons are the only thing that robots can reason about.
So when setting velocity in the physics engine, we'd have to divide appropriately.

## LOG Instruction

We really need this.
But it requires string literal arguments, which we don't yet know how to tokenize.

## Wall Problem

We check the wall register and adjust speed accordingly.
However, that change of course doesn't take effect until the next update cycle.
In the meantime, we loop around and check the wall register again.
It hasn't changed, we haven't moved, and we change our course again!

So no wonder everyone is killing themselves on walls.

Should reading the wall register automatically reset it?

The other choice is to call `SYNC` right away, but that seems like a poor idea: you could still look around and shoot things.

Ultimately, the *correct* way to deal with walls is to use interrupts.
But it should always be the case that you can do it on your own.

I guess really it's up to the program to check the wall and its heading before making a change.

*Clearing the register* on a read is not necessarily a bad idea.
It would be really odd to need to read it twice, and if you knew you might do that, then you could always save the value in a global (though you'd then have to unset the global yourself).

## AIM and LOOK

Basically, AIM is where your gun is pointing, LOOK is where your eyes are pointing. So it's a little funny that you can "see" through the AIM register.

In fact, in RW you *can't*. You don't see through the LOOK register either.
You see through the RANGE register.
LOOK is just an offset from AIM, and that's what determines the contents of RANGE.

In the engine, the implementation is slightly different.
There is a `look` register that has several components: `angle`, `dist`, and `thing` (which gives you full knowledge of the closest thing you can see, which is overkill, and has to be treated with respect).
But I think `look.angle` has to be the sum of RoboWar AIM and LOOK.

Philosophically, I am not sure I want to buy into the RoboWar vision hardware.
I'm guessing that was patched in later, and made more complicated by the fact that registers can't store structured data.
I want the bot's "eyes" to be fully independent. But it should also be possible to advance with your gun up, viewing the field through your gunsights.
So you can use your eyes normally, and then bring your gun up and aim where you were looking, or you can use your gunsight for glasses, so to speak.

That actually is close to the RW model: You can see through your `aim`, or through an independent `look`. Only I hadn't thought of `look` as being tied to `aim`, as an offset.
But that's not a terrible idea: it kind of prevents you from swivelling your head 360 degrees while you aim a shot behind you.

So how could you model both scenarios? 

* You should be able to set `aim` without initiating a raycast, I think.
  So there should be a separate way to say "Look through `aim`".
* In RW, setting `aim` automatically sets `look`, in the sense that the 
  offset (the contents of the `look` register) doesn't change, but the base angle does.
* In RW, setting AIM actually does initiate a raycast.

So the conclusion is, yeah, setting `aim` causes a raycast along the aim plus look angle.

So if you want to see the world through your sights, keep `look` to zero.

In our terms, I guess the main vision structure has to support an offset, then.
```
sight: {
	angle: (radians),
	offset: (radians),
	dist: (pixels | -1),
	thing: (object),
}
```
For now, we will be dealing without a separate LOOK register, so `offset` should always be zero.

## Projectiles

A projectile is an object with a drive vector and a payload, and no real intelligence.

It is destroyed on collision with anything else (including another projectile).
But in the process it may do damage to the thing it collided with.
And/or maybe also in an *area of effect*.

It's initial position is right on the edge of the bot circle.
Maybe if we draw them *under* the bots we can fudge cases where the sprite is bigger than the actual game object?
Anyhow, so we have to get a vector from the radius and the aim angle, but that's not a big deal.

Speed is set according to the *projectile type*.
Each projectile has an energy payload that is delivered through its detonation method, I guess. Or maybe more generally "impact".

* An arrow or bullet, on impact, causes simple damage to the object it 
  collides with (unless it's another projectile, in which case it destroys it).
  I suppose a magic missile or a blaster pulse would behave pretty much the same.
* A fireball or an explosive bullet might do less direct damage, and more 
  blast damage, in an area of effect.
* A poisoned arrow or poison spell would somehow *attach* to the target, with 
  all weapons attached to a given agent doing damage over time.
  So that would have to be checked at start-of-chronon.

I guess all buffs and debuffs would also act through this *attachment* mechanism. 

Someday we will want impacts to have physical effects as well, being capable of knocking back an agent. In order for that to happen, they need mass.
So *density* might vary among projectile types, assuming their size will remain fairly constant.
Also impact would vary with speed, so that's a factor.
But projectiles can't move *too* fast, or we will have to deal with tunnelling!

For practical purposes, I guess that means they can't move more than 30 pixels. 20, if we are worried about tunnelling through walls.

If projectiles take part in the *collision system* then they must have physical bodies. And they must have *event handlers*.

And obviously they need sprites. 

Also they come in multiple states, in case there is any after-impact animation to do. For now they just disappear, but later even bullets might want to cause a brief splatter.

Immediate damage, and state-change if called for, is done in the impact handler. This is really the "payload logic" for the weapon.

## Removing Projectiles

There's a big long list of these things in the Game object collection.
How do we find the one to remove?

I guess they have to have unique IDs of some kind.
Maybe of the form `shooter_counter`.
Bot names don't have to be identifiers, though, so some character substitution might be necessary.
Maybe assign bots unique IDs too, and maintain a global map of IDs to names...

They need to go in a collection that we can iterate over.
Otherwise, I'd say map from IDs to objects, and erase the key entry to remove it.
```
Object.keys(obj).forEach(function(key,index) {
	game.objects.projectiles[key].update()...
});

Object.entries(game.objects.projectiles)  // --> [[id,obj], ...]
	.map((kv) => kv[1].update())          // --> [[task,...], ...]
	.reduce((ts, t) => ts.concat(t), [])  // --> [task,...]
	.forEach((tsk) => this.execute(tsk));
```

## Projectile Logic

### Shooting your own bullets

If we record the *shooter* of each projectile, maybe we can make a rule that your own bullets' collisions are ignored?
Still have to explain that to the physics engine, of course.

### Bullets are actually rays

Also, maybe we can model all projectiles as underlyingly *rays*.
And then they are rendered according to how long they've been around.
So you basically have a 0-width line with a little round "bulge" in it somewhere. 

That might help with *tunnelling*. 
Projectiles would be removed from the physics system, probably.
But you'd know if a bullet's ray intersected with the target, and then it's just a question of knowing when it would hit the target, and then seeing if the target is still there at that time.

But then how would you deal with some other robot jumping into the line of fire?

1. When a projectile is fired, we schedule some impact checks for later.
2. On chronon-start, after everything has moved, we check to see if anything
   has moved into the line of fire of a projectile.

That's a lot of projectile lines-of-fire to check. And what about tunneling in a case like that?

### Superimposing bullets

Suppose instead of firing a bullet every time a bot stores to `fire`,
we just *accumulate* the stored energy into a single bullet, and fire that at the end of the bot's turn?

Then keeping your own bullets from destroying each other suddenly becomes a lot easier.

Also the number of objects we have to keep track of becomes more manageable.

# 2017-09-13

## Bullet Collisions

There is a problem with stacking bullets: Now an agent cannot fire a *spread* of shots during a single turn. Really what we want is to prevent more than one shot at a single angle.

I guess that would mean that whenever we change aim, 
any energy stored in `fire` would get launched (at the old aim angle).

That seems a little flaky from an engineering standpoint, though.
Wandering a little far off of the semantic hard-line...

Note that a narrow spread of bullets would still probably lead to initial collisions. They are too big not to overlap if launched only a few degrees apart.

It would still be possible to add a `shooter` field to projectiles, and simply ignore collisions between projectiles from the same shooter.
They would still have physical collisions, but they wouldn't die on collisions with their own fellows.

> I wonder if pushing out multiple bullets into the same physical space 
  could lead to them pilling up "in the barrel of the gun" so to speak, and having collisions with the shooter? I don't *think* I've seen this happen, but it would be hard to see.

It is possible to use `Body.collisionFilter.group` to make physical bodies not collide. 
So if each agent has an index (like in its name, for generic actors),
and we negate that and assign it to its collision filter group,
then a given body's bullets will not collide with each other, but will collide with other bodies' bullets.

### Standoff

It turns out to be very common that if two bots are shooting at each other, their bullets will collide, leading to a standoff.

So shooting down the enemy's bullets turns out to be too easy, and not really helpful to the game.

## Programming Agents

I'm not sure M/VML is a real improvement over a stack based language for programming. Maybe it's just my habits. But it would be really nice to at least be able to write

* complex expressions
* if conditions

I guess the second is maybe a special case of the first.

At this point I do not find myself obsessing over chronon breaks, but then I'm not doing any tracking, or handling interrupts, or really anything sophisticated. 
But it seems like a high level language and some seat-of-the-pants "profiling" is probably what most people would want.


# 2017-09-14

## Frames per Chronon

We probably want to be able to slow the game down.
To do that, we need the physics engine to pretend that all velocities are some fraction of what the game thinks they are.
If we are running at 2-1, then velocities are cut in half. 
In general, I guess the formula is "game velocity over frames", or times the inverse (chronons per frame).

Is it just velocities?
That would mean we would have to patch this factor into all `setVelocity` calls, and never set drive vectors directly.

Or maybe just introduce a *shim* in the physics wrapper.
So never call `body.setVelocity(drv)`, 
always call `physics.setVelocity(body, drv)`.

Actually, any attempt to access `object.body` outside of the physics module should be considered a bug.

Anyway, once we have velocities down to scale, 
we then have to scale back the processor.
Basically, *every call to `update` is now half a call to `update`*.

That's more of a problem, because we have to factor out all code that is called during a chronon break somehow.

Or maybe just don't call the bot's interpreter on off-frames?

Then there's the problem of *events*.
The physics engine is still advancing, and it may report collisions on off-frames.
If they aren't handled promptly, the animation may look weird.
At the very least, we have to make sure that game events persist until they are handled, even across calls to `update`.

Events can do two things:

1. Have game-logic level effects (e.g. damage, stasis) on the target of an
   impact.
2. Trigger interrupts that affect agent level logic.

The game effects can be assessed immediately.
Any interrupts (including those triggered by game effects) would get added to the interrupt queue.
Again, that can't be emptied until it has been processed, but I believe that is already what happens now.
